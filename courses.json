[
  {
    "duration": 76,
    "description": "Learn to post-train and customize an LLM in this short course, “Post-training of LLMs,” taught by Banghua Zhu, Assistant Professor at the University of Washington, and co-founder of NexusFlow. Before a large language model can follow instructions or answer questions, it undergoes two key stages: pre-training and post-training. In pre-training, it learns to predict the next word or token from large amounts of unlabeled text. In post-training, it learns useful behaviors such as following instructions, tool use, and reasoning. Post-training transforms a general-purpose token predictor—trained on trillions of unlabeled text tokens—into an assistant that follows instructions and performs specific tasks. In this course, you’ll learn three common post-training methods—Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Online Reinforcement Learning (RL)—and how to use each one effectively. With SFT, you train the model on input-output pairs with ideal output responses. With DPO, you provide both a preferred (‘chosen’) and a less preferred (‘rejected’) response, and train the model to favor the preferred output. With RL, the model generates an output, receives a reward score based on human or automated feedback, and updates the model to improve performance. You’ll learn the basic concepts, common use-cases, and principles for curating high-quality data for effective training in each of these methods. Through hands-on labs, you’ll download a pre-trained model from HuggingFace and post-train it using SFT, DPO, and RL to see how each technique shapes model behavior.",
    "url": "https://www.deeplearning.ai/short-courses/post-training-of-llms",
    "title": "Post-training of LLMs",
    "studentProfile": "This course is for AI builders who want to adapt language models for specific tasks or behaviors. If you’re familiar with LLM basics and ready to go beyond pre-training, this course will help you understand and apply the key techniques that make LLMs truly useful.",
    "learningGoals": [
      "Understand when and why to use post-training methods like Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Online Reinforcement Learning.",
      "Learn the concepts underlying the three post-training methods of SFT, DPO, and Online RL, their common use-cases, and how to curate high-quality data to effectively train a model using each method.",
      "Download a pre-trained model and implement post-training pipelines to turn a base model into an instruct model, change the identity of a chat assistant, and improve a model’s math capabilities."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Banghua Zhu",
        "title": "Assistant Professor at the University of Washington, Principal Research Scientist at Nvidia, Co-founder of Nexusflow"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Introduction to Post-training",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Basics of SFT",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "SFT in Practice",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Basics of DPO",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "DPO in Practice",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Basics of Online RL",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Online RL in Practice",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips, Help, and Downlad",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 101,
    "description": "Introducing ACP: Agent Communication Protocol, a short course built in partnership with IBM Research’s BeeAI and taught by Sandi Besen, AI Research Engineer & Ecosystem Lead at IBM, and Nicholas Renotte, Head of AI Developer Advocacy at IBM. The course teaches how to build ACP-compliant agents, chain them in workflows, and use a registry for easy discovery and sharing. It covers the architecture, lifecycle, and practical applications of ACP agents and workflows.",
    "url": "https://www.deeplearning.ai/short-courses/acp-agent-communication-protocol",
    "title": "ACP: Agent Communication Protocol",
    "studentProfile": "This course is perfect for AI builders who want to easily reuse and connect multiple agents built with different frameworks in a single system, or for anyone curious to learn about the Agent Communication Protocol. Some experience with Python is recommended.",
    "learningGoals": [
      "Build ACP-compliant agents by wrapping them in an ACP server, launch the server to activate the agents, and make them discoverable by ACP clients to enable easy integration within multi-agent systems.",
      "Chain ACP-compliant agents in linear and hierarchical workflows; use a router agent to delegate tasks to the specialized agents.",
      "Import ACP-compliant agents into a registry to make them easy to discover and share across teams."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Sandi Besen",
        "title": "AI Research Engineer and Ecosystem Lead at IBM Research"
      },
      {
        "name": "Nicholas Renotte",
        "title": "Head of AI Developer Advocacy at IBM"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Why Agent Communication protocol",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "ACP Core Principles",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "Building a RAG Agent with CrewAI",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Wrapping the RAG Agent into an ACP Server",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Calling an ACP Agent using the Client",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Wrapping a Smolagents Agent into an ACP Server",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Sequentially Chaining the Agent Calls",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Hierarchically Chaining the Agent Calls using a Router Agent",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Adding MCP to the Hospital Server",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Managing ACP Compliant Agents",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Resources, Tips, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 60,
    "description": "Introducing Building with Llama 4, a short course created in collaboration with Meta and taught by Amit Sangani, Director of Partner Engineering for Meta’s AI team. The course focuses on the new Llama 4 models, including the Mixture-of-Experts (MOE) architecture, and covers building applications with the official API. Students will learn to work with long-context windows, multi-modal capabilities such as image reasoning and grounding, prompt optimization, and the synthetic data kit for high-quality datasets. The course includes hands-on lessons to build apps using Llama 4’s features and tools.",
    "url": "https://www.deeplearning.ai/short-courses/building-with-llama-4",
    "title": "Building with Llama 4",
    "studentProfile": "Anyone who wants hands-on experience building with the Llama 4 family of models.",
    "learningGoals": [
      "Get hands-on with Llama 4 family of models, understand its Mixture-of-Experts (MOE) architecture, and how to build applications with its official API.",
      "Apply Llama 4’s capabilities across multi-image reasoning, image grounding to identify objects and their bounding boxes, and querying over long-context texts of up to 1 million tokens.",
      "Use Llama 4’s prompt optimization tool to automatically refine system prompts and its synthetic data kit to create high-quality datasets for fine-tuning."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Amit Sangani",
        "title": "Senior Director of Partner Engineering of Meta"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Overview of Llama 4",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Quickstart with Llama 4 and API",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Image Grounding",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Llama 4 Prompt Format",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Long-Context Understanding",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Prompt Optimization Tool",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Synthetic Data kit",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips, Help, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 88,
    "description": "Learn to build and orchestrate a RAG pipeline in Orchestrating Workflows for GenAI Applications, built in partnership with Astronomer and taught by Kenten Danas and Tamara Fingerlin. This course teaches you how to turn a Retrieval Augmented Generation (RAG) prototype into a robust, automated pipeline using Airflow 3.0. You'll learn to build workflows for ingesting and embedding book description texts into a vector database and querying that database to recommend books. Key skills include creating pipelines with modular tasks, scheduling using time-based and data-aware triggers, dynamic task mapping for parallel tasks, and adding retries and notifications for failure handling. By the end, you will be able to design, build, and automate GenAI workflows ready for production.",
    "url": "https://www.deeplearning.ai/short-courses/orchestrating-workflows-for-genai-applications",
    "title": "Orchestrating Workflows for GenAI Applications",
    "studentProfile": "This course is for AI builders who want to automate and deploy their GenAI prototypes more reliably. No Airflow experience is required, just familiarity with Python and an interest in moving to production-ready workflows.",
    "learningGoals": [
      "Orchestrate a RAG prototype using Airflow: transform your code into pipelines consisting of modular tasks and schedule them using time-based and data-aware scheduling.",
      "Apply dynamic task mapping to run tasks efficiently in parallel and automatically adapt to new data sources.",
      "Build robust pipelines by adding automatic retries and failure notifications to handle errors gracefully."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Kenten Danas",
        "title": "Senior Manager, Developer Relations at Astronomer"
      },
      {
        "name": "Tamara Fingerlin",
        "title": "Developer Advocate at Astronomer"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "From Notebook To Pipeline",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Your RAG Prototype",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Building a Simple Pipeline",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Turning your Prototype into a Pipeline",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Scheduling and Dag Parameters",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Make the Pipeline Adaptable",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Prepare to Fail",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "GenAI pipelines in Real Life",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "Optional: How to Set up a Local Airflow Environment",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Resources, Help, and Downloads",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 49,
    "description": "Join DSPy: Build and Optimize Agentic Apps, built in partnership with Databricks and taught by Chen Qian, a software engineer at Databricks and co-lead maintainer of the DSPy framework. This course teaches you how to use DSPy to build and optimize LLM-powered applications. You'll write programs using DSPy's signature-based programming model, debug them with MLflow tracing, and automatically improve their accuracy with DSPy Optimizer. You'll learn to build modular, traceable, and debuggable GenAI agentic applications by chaining DSPy modules like Predict, ChainOfThought, and React, and using MLflow for tracing and debugging. The course also covers optimizing GenAI apps with DSPy Optimizer for prompt tuning and improving few-shot examples to enhance answer accuracy and consistency. The course includes practical coding exercises, tracing with MLflow, and optimization techniques to build structured, robust, and adaptable GenAI applications.",
    "url": "https://www.deeplearning.ai/short-courses/dspy-build-optimize-agentic-apps",
    "title": "DSPy: Build and Optimize Agentic Apps",
    "studentProfile": "This course is ideal for anyone who wants a more reliable, maintainable way to build and debug multi-step agents. No prior DSPy experience or knowledge is required.",
    "learningGoals": [
      "Learn the fundamentals of DSPy and how to use its signature and module-based programming model to build modular, traceable, and debuggable GenAI agentic applications.",
      "Build agentic applications by chaining DSPy modules like Predict, ChainOfThought, and ReAct, and use MLflow to trace and debug your programs.",
      "Optimize your GenAI apps with DSPy Optimizer by automating prompt tuning and improving few-shot examples to improve answer accuracy and consistency."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Chen Qian",
        "title": "Software Engineer at Databricks and co-lead maintainer of the DSPy framework"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Introduction to DSPy",
        "usesCodeExample": false
      },
      {
        "duration": 17,
        "title": "DSPy Programming - Signatures and Modules",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Debug Your DSPy Agent with MLflow Tracing",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Optimizing Agents with DSPy Optimizer",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips, Help, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 60,
    "description": "Join Reinforcement Fine-Tuning LLMs with GRPO, built in collaboration with Predibase, and taught by Travis Addair, its Co-Founder and CTO, and Arnav Garg, its Senior Engineer and Machine Learning Lead. This course covers reinforcement fine-tuning (RFT) techniques using the Group Relative Policy Optimization (GRPO) algorithm for improving reasoning in large language models (LLMs). Learn to design reward functions, use LLM as a Judge for subjective tasks, avoid reward hacking, calculate the loss function in GRPO, and launch RFT jobs using Predibase’s hosted training services.",
    "url": "https://www.deeplearning.ai/short-courses/reinforcement-fine-tuning-llms-grpo",
    "title": "Reinforcement Fine-Tuning LLMs with GRPO",
    "studentProfile": "This course is for anyone who wants to fine-tune LLMs for complex reasoning tasks without relying on large labeled datasets. Ideal for those interested in reinforcement learning, LLM reasoning, and improving the performance of small, open-source models.",
    "learningGoals": [
      "Learn the foundations of reinforcement learning and how to use the Group Relative Policy Optimization (GRPO) algorithm to improve reasoning in large language models.",
      "Design effective reward functions, and learn how rewards are converted into advantages to steer models toward high-quality behavior across multiple use cases.",
      "Learn to use LLM as a Judge for subjective tasks, overcome reward hacking with penalty functions, and calculate the loss function in GRPO."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Travis Addair",
        "title": "Co-Founder and CTO at Predibase"
      },
      {
        "name": "Arnav Garg",
        "title": "Senior Machine Learning Engineer at Predibase"
      }
    ],
    "courseItems": [
      {
        "duration": 60,
        "title": "Reinforcement Fine-Tuning LLMs with GRPO",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": false
  },
  {
    "duration": 98,
    "description": "Join MCP: Build Rich-Context AI Apps with Anthropic, a short course created in partnership with Anthropic and taught by Elie Schoppik. Connecting AI applications to external systems to bring rich context to LLMs has often meant writing custom integrations for each use case. This has fragmented AI development between teams within a company and across the industry. The Model Context Protocol (MCP) is an open protocol that standardizes how LLMs access tools, data, and prompts from external sources, simplifying how new context is integrated into AI applications. MCP, developed by Anthropic, is based on a client-server architecture. It defines the communication details between an MCP client, hosted inside the AI application, and an MCP server that exposes tools, resources, and prompt templates. The server can be a subprocess launched by the client and running locally, or an independent process running remotely. In this hands-on course, you’ll learn the core concepts of MCP and how to implement it in your AI Application. You’ll make a chatbot MCP-compatible, build and deploy an MCP server, and connect the chatbot to your MCP server and other open-source servers. By the end of the course, you’ll be able to build rich-context AI applications that can connect to a growing ecosystem of MCP servers, with minimal integration work.",
    "url": "https://www.deeplearning.ai/short-courses/mcp-build-rich-context-ai-apps-with-anthropic",
    "title": "MCP: Build Rich-Context AI Apps with Anthropic",
    "studentProfile": "It’s helpful to be familiar with Python and have a basic understanding of LLM prompting and LLM application development.",
    "learningGoals": [
      "Explore how MCP standardizes access to tools and data for AI applications, its underlying architecture, and how it simplifies the integration of new tools and connections to external systems (e.g., GitHub repos, Google Docs, local files).",
      "Build and deploy an MCP server that provides tools, resources, and prompts, and add it to the configuration of AI applications, such as Claude Desktop, to extend them.",
      "Build an MCP-compatible application that hosts multiple MCP clients, each maintaining 1-to-1 connection to an MCP server."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Elie Schoppik",
        "title": "Head of Technical Education at Anthropic"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Why MCP",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "MCP Architecture",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Chatbot Example",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Creating an MCP Server",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Creating an MCP Client",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Connecting the MCP Chatbot to Reference Servers",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Adding Prompt and Resource Features",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Configuring Servers for Claude Desktop",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Creating and Deploying Remote Servers",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 50,
    "description": "Join Building AI Voice Agents for Production, created in collaboration with LiveKit and RealAvatar, and taught by Russ d’Sa, Shayne Parmelee, and Nedelina Teneva. The course covers core architecture of voice agents including STT, LLMs, and TTS, building and deploying low-latency voice agents, measuring and optimizing latency, and making voice agents natural and scalable. It incorporates voice technology from ElevenLabs and guides learners through building a voice agent pipeline and deploying it on cloud infrastructure.",
    "url": "https://www.deeplearning.ai/short-courses/building-ai-voice-agents-for-production",
    "title": "Building AI Voice Agents for Production",
    "studentProfile": "Anyone who wants to build conversational voice applications using LLMs. Familiarity with basic Python and foundational AI workflows is recommended to get the most out of this course.",
    "learningGoals": [
      "Understand the core architecture of voice agents, including the trade-offs between modular pipelines and real-time APIs, and how components like STT, LLMs, and TTS work together.",
      "Build and deploy a voice agent that handles speech input, generates LLM responses, and replies using custom voices while managing latency and user interruptions.",
      "Measure and optimize latency across your voice pipeline, and apply strategies to make your agent feel more natural, responsive, and scalable in real-world settings."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Russ d’Sa",
        "title": "CEO of LiveKit"
      },
      {
        "name": "Shayne Parmelee",
        "title": "Developer Advocate at LiveKit"
      },
      {
        "name": "Nedelina Teneva",
        "title": "Head of AI at RealAvatar"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "Voice Agent Overview",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "End-to-end architecture - Part 1",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "End-to-end architecture - Part 2",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Voice Agent Components",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Optimizing Latency",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix-Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 82,
    "description": "Learn how to build agentic memory into your applications in this short course, LLMs as Operating Systems: Agent Memory, created in partnership with Letta, and taught by its founders Charles Packer and Sarah Wooders. The course covers building agents with long-term, persistent memory using Letta to manage and edit context efficiently. It teaches how an LLM agent can act as an operating system to manage memory, autonomously optimizing context use, and how to apply memory management to create adaptive, collaborative AI agents for real-world tasks like research and HR.",
    "url": "https://www.deeplearning.ai/short-courses/llms-as-operating-systems-agent-memory",
    "title": "LLMs as Operating Systems: Agent Memory",
    "studentProfile": "Anyone who has basic Python skills and is curious about how autonomous agents can manage their own memory.",
    "learningGoals": [
      "Build agents with long-term, persistent memory using Letta to manage and edit context efficiently.",
      "Learn how an LLM agent can act as an operating system to manage memory, autonomously optimizing context use.",
      "Apply memory management to create adaptive, collaborative AI agents for real-world tasks like research and HR."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Charles Packer",
        "title": "Co-Founder of Letta"
      },
      {
        "name": "Sarah Wooders",
        "title": "Co-Founder of Letta"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Editable memory",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Understanding MemGPT",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Building Agents with Memory",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Programming Agent Memory",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Agentic RAG and External Memory",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Multi-agent Orchestration",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips, Help, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 44,
    "description": "Learn how to build code agents in Building Code Agents with Hugging Face smolagents, created in collaboration with Hugging Face, and taught by Thomas Wolf, co-founder and CSO, and Aymeric Roucher, Project Lead. Tool-calling agents use large language models to write out multiple function calls sequentially to complete a complex sequence of tasks. They generate one function call, execute it, observe, reason, and then decide what to do next. Code agents take a different approach. They consolidate all these calls into a single block or snippet of code, letting the LLM lay out an entire plan of action at once. That block can be executed efficiently, providing more reliable results. In this short course, you’ll learn how to build your own code agents using smolagents, a lightweight agentic framework from Hugging Face. Along the way, you’ll explore real-world use cases, learn how to run LLM-generated code safely, and build an evaluation system that optimizes your code agent for production. By the end of this course, you’ll know how to build and run code agents using smolagents, understand their advantages over tool-calling agents, and be ready to deploy them safely with a structured evaluation system in your projects.",
    "url": "https://www.deeplearning.ai/short-courses/building-code-agents-with-hugging-face-smolagents",
    "title": "Building Code Agents with Hugging Face smolagents",
    "studentProfile": "This course is for anyone interested in agentic workflows, especially those looking to go beyond tool-calling and experiment with LLMs that plan and execute tasks autonomously. Basic Python knowledge and familiarity with LLMs is recommended.",
    "learningGoals": [
      "Understand the architecture and benefits of code agents that write code to perform tasks, compare them to traditional tool-calling agents, and learn when to use each approach.",
      "Build and deploy secure code agents using Hugging Face’s smolagents, including techniques for sandboxing and safe execution of LLM-generated code.",
      "Design, monitor, and evaluate single and multi-agent systems capable of handling tasks like web browsing, data extraction, and multi-step reasoning."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Thomas Wolf",
        "title": "Co-founder and CSO of Hugging Face"
      },
      {
        "name": "Aymeric Roucher",
        "title": "Project Lead at Hugging Face"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "A Brief History of Agents",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Introduction to Code Agents",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Secure Code Execution",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Monitoring and Evalutating your Agent",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Build a Deep-Research Agent",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips and Helps",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 55,
    "description": "Learn how to build AI agents that interact with websites in Building AI Browser Agents, taught by Div Garg and Naman Garg, Co-founders of AGI Inc, and built in partnership with AGI Inc. AI browser agents can log into websites, fill out forms, click through web pages, or even place an online order for you. They use both visual information, like screenshots, and structural data, like the HTML or Document Object Model (DOM) of a web page, to reason and take actions. With the complexity of web pages and many possible actions at each step, it can be challenging for an AI browser agent to complete an assigned task. A single error—like clicking the wrong button or misreading a field—can compound into unexpected outcomes. In this course, you’ll understand how autonomous web agents work, their current limitations, and how AgentQ enables them to improve through self-correction. You will learn what web agents are, how they automate tasks online, their architecture, key components, limitations, and an overview of their decision-Making strategies. You will build web agents that can scrape websites and execute multiple tasks, and explore AgentQ framework that enables agents to self-correct through techniques such as Monte Carlo Tree Search (MCTS). By the end of this course, students will have hands-on experience building browser agents and a deeper understanding of how to make them more robust and reliable.",
    "url": "https://www.deeplearning.ai/short-courses/building-ai-browser-agents",
    "title": "Building AI Browser Agents",
    "studentProfile": "This course is ideal for learners with basic Python skills who want to explore how to build autonomous agents that interact with the web.",
    "learningGoals": [
      "Learn the fundamentals of autonomous web agents, what they are, how they work, their limitations, and the decision-making strategies taken to optimize their performance.",
      "Build autonomous web agents that can perform tasks such as finding, scraping, and summarizing a webpage, filling out forms, and signing up for newsletters.",
      "Explore the AgentQ framework, which uses a combination of Monte Carlo Tree Search (MCTS), self-critique mechanism, and Direct Preference Optimization (DPO) to teach agents to self-correct."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Div Garg",
        "title": "Co-founder of AGI Inc"
      },
      {
        "name": "Naman Garg",
        "title": "Co-founder of AGI Inc"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Intro to Web Agents",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Building a Simple Web Agent",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Building an Autonomous Web Agent",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Agent Q",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Deep Dive into AgentQ and MCTS",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Future of AI Agents",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 71,
    "description": "Welcome to Getting Structured LLM Output, built in partnership with DotTxt, and taught by Will Kurt, Founding Engineer, and Cameron Pfiffer, Developer Relations Engineer at DotTxt. When building production-ready software, it’s challenging to parse through and rely on freeform text outputs. Structured outputs—like JSON—solve this by converting natural language into consistent, clear, and programmable data that a machine can read and process. In this course, you’ll learn how to generate structured outputs while building several use cases, including a social media analysis agent. You’ll gain a fundamental understanding of structured outputs and learn efficient ways to generate outputs in your defined schema or format. You’ll begin by using structured output APIs, then follow it up by utilizing re-prompting libraries like “instructor” to generate structured output. Afterward, you’ll learn how constrained decoding works, in which constraints are applied on each subsequent token generated, blocking any tokens that don’t fit your defined schema. In detail, you’ll: Learn why structured outputs are important, how they allow for scalable software development, and the different approaches to generate them, including vendor-provided APIs, re-prompting libraries, and structured generation. Build a simple social media agent using OpenAI’s structured output API, learn how to define a model’s desired structured output using Pydantic, and perform basic programming with your outputs, such as importing structured data into a data frame using pandas. Learn how to use the open-source library “_instructor,_” which checks the structured output of the model and re-prompts the model until it validates the desired output, and explore the limitations of this approach. Understand how structured generation by “_outlines”_ works by modifying LLM logits, per token generated based on instruction, to give a particular output structure. Learn how regular expressions, which power outlines, are represented as finite-state machine, and how they can be used to develop a range of structured output beyond JSON. By the end of this course, you’ll have broadened your knowledge of the approaches you can use to get structured outputs from your LLM applications.",
    "url": "https://www.deeplearning.ai/short-courses/getting-structured-llm-output",
    "title": "Getting Structured LLM Output",
    "studentProfile": "It’s helpful to be familiar with Python, the basics of LLM prompting, and LLM application development.",
    "learningGoals": [
      "Get an overview of structured output generation, its importance, and the different approaches to generating them.",
      "Build a social media agent using structured output and learn how to use re-prompting libraries like instructor.",
      "Understand the concepts behind constrained decoding and how the LLM logits are modified to get a particular output structure."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Will Kurt",
        "title": "Founding Engineer of DotText"
      },
      {
        "name": "Cameron Pfiffer",
        "title": "Developer Relations Engineer at DotTxt"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "Introduction to Structured Output Generation",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "How To Use Structured Outputs",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Retry-based Structured Output",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Structured Generation with Outlines",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Structured Generation: Beyond JSON",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix-Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 94,
    "description": "Learn to build and host applications with an AI agent in Vibe Coding 101 with Replit, built in partnership with Replit and taught by Michele Catasta, President, and Matt Palmer, Head of Developer Relations. This course covers the principles of agentic code development, effective AI collaboration, and hands-on building and deployment of web applications with AI coding agents using Replit's cloud environment. By the end of the course, learners will have a solid foundation in building with coding agents and a process for effective vibe coding.",
    "url": "https://www.deeplearning.ai/short-courses/vibe-coding-101-with-replit",
    "title": "Vibe Coding 101 with Replit",
    "studentProfile": "Anyone can join! It's helpful to have some coding experience and some background in prompting LLMs. To complete the course, you will need to sign up for a free Replit account.",
    "learningGoals": [
      "Build and share two applications—a website performance analyzer and a voting app—while using an AI coding agent to debug, customize, and strengthen your coding skills.",
      "Learn the principles of agentic code development and skills to effectively build, host, and share your apps with Replit coding agents and assistants.",
      "Use product requirement documents, wireframes, and good prompting practices to prototype, debug, and iterate your applications."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Michele Catasta",
        "title": "President of Replit"
      },
      {
        "name": "Matt Palmer",
        "title": "Head of Developer Relations at Replit"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Principles of Agentic Code Development",
        "usesCodeExample": false
      },
      {
        "duration": 23,
        "title": "Planning and Building an SEO Analyzer",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Implementing SEO Analysis Features",
        "usesCodeExample": false
      },
      {
        "duration": 26,
        "title": "Planning and Building a Voting App",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Enhancing the National Parks Voting App",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Next steps and Best Practices",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": false
  },
  {
    "duration": 64,
    "description": "Learn to build an agent with long-term memory in Long-Term Agentic Memory with LangGraph! Created in partnership with LangChain, and taught by its Co-Founder and CEO, Harrison Chase.\n\nThe course covers how memory works and how the three types of memory – semantic, episodic, and procedural – are used in agentic workflows. Learners will build a personal email agent with routing, writing, and scheduling tools to automatically ignore, respond to, or notify about incoming emails. The course also teaches how to add long-term memory to agents by adding facts, user preferences, and evolving system prompts to a memory store that can be searched for ongoing interactions.\n\nBy the end, learners will have the foundational mental framework to build an agent with long-term memory using LangGraph.",
    "url": "https://www.deeplearning.ai/short-courses/long-term-agentic-memory-with-langgraph",
    "title": "Long-Term Agentic Memory with LangGraph",
    "studentProfile": "It’s helpful to be familiar with Python and have a basic understanding of LLM prompting and LLM application development.",
    "learningGoals": [
      "Learn how memory works and how the three types of memory – semantic, episodic, and procedural – are used in agentic workflows.",
      "Build a personal email agent with routing, writing, scheduling tools to automatically ignore, respond to, or notify about incoming emails.",
      "Add long-term memory to your agent by adding facts, user preferences, and evolving system prompts to a memory store that can be searched for on-going interactions."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Harrison Chase",
        "title": "Co-Founder and CEO of LangChain"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Introduction to Agent Memory",
        "usesCodeExample": false
      },
      {
        "duration": 16,
        "title": "Baseline Email Assistant",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Email Assistant with Semantic Memory",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Email Assistant with Semantic + Episodic Memory",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Email Assistant with Semantic + Episodic + Procedural Memory",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips and Helps",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 59,
    "description": "Learn to build Event-Driven Agentic Document Workflows, in this new course, built in partnership with LlamaIndex, and taught by Laurie Voss, VP of Developer Relations. Agentic document workflows are agent-based applications that automate end-to-end document processing workflows using an event-driven architecture. The course covers designing workflows that parse forms, retrieve and structure data from source documents using RAG, incorporate human-in-the-loop feedback via text and voice, and handle branching and concurrent executions.",
    "url": "https://www.deeplearning.ai/short-courses/event-driven-agentic-document-workflows",
    "title": "Event-Driven Agentic Document Workflows",
    "studentProfile": "Anyone who has basic Python knowledge and basic understanding of object-oriented programming, and wants to learn to build event-driven agent workflows.",
    "learningGoals": [
      "Learn to build event-driven workflows with complex branching, looping logic, and concurrent executions.",
      "Build an event-driven agentic workflow that parses forms, retrieves relevant information from a source document using RAG, and returns answers to the form’s fields as structured outputs.",
      "Incorporate human-in-the-loop to a document workflow to provide feedback to the agent through text and voice instructions."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Laurie Voss",
        "title": "VP of Developer Relations at LlamaIndex"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "What are Agentic Document Workflows",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Building a Workflow",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Adding RAG",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Form Parsing",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Human in the Loop",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Use your Voice",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 70,
    "description": "Join our latest course Build Apps with Windsurf’s AI Coding Agents, built in partnership with Windsurf and taught by Anshul Ramachandran. AI-assisted Integrated Development Environments, or IDEs, make coding and development workflows faster, more efficient, and more fun. Agentic tools like Windsurf are collaborative coding agents that help break down complex applications, iterate efficiently, and generate code across multiple files. This course teaches how to use Windsurf to build applications, debug JavaScript code, update large codebases, and build a Wikipedia analysis app with caching and full-stack visualization, while learning to manage unexpected AI behavior.",
    "url": "https://www.deeplearning.ai/short-courses/build-apps-with-windsurfs-ai-coding-agents",
    "title": "Build Apps with Windsurf’s AI Coding Agents",
    "studentProfile": "It’s helpful for anyone familiar with coding to have a basic understanding of LLM prompting and AI-assisted application development.",
    "learningGoals": [
      "Learn how to collaborate with AI agents to streamline coding workflows, generate and modify code across multiple files, and enhance development efficiency.",
      "Understand how AI coding agents combine tool usage, context awareness, and human-action tracking to function effectively and get a deep dive into search and discovery as one of the main challenges of AI coding agents.",
      "Use Windsurf to do real-world coding tasks, including debugging JavaScript code, updating an old large codebase, and building a Wikipedia analysis app that retrieves, processes, and visualizes data while learning to manage unexpected AI behavior."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Anshul Ramachandran",
        "title": "Head of Enterprise & Partnerships at Codeium"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Getting Started & First App",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "AI Code Assistants 101",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "Fixing Tests Automatically",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "How a Collaborative AI Code Agent Works",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Search & Discovery for AI Agents",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Understanding Large Codebases",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Wikipedia Analysis App – Data Analysis",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Wikipedia Analysis App – Caching",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Wikipedia Analysis App – Fullstack App",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Wikipedia Analysis App – Polish",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Prompts and Repos",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 136,
    "description": "Learn how to systematically assess and improve your AI agent’s performance in Evaluating AI Agents, a short course built in partnership with Arize AI and taught by John Gilhuly, Head of Developer Relations, and Aman Khan, Director of Product. When you’re building an AI Agent, an important part of the development process is evaluations or evals. Whether you’re building a shopping assistant, coding agent, or research assistant, having a structured evaluation process helps you refine its performance systematically—rather than relying on trial and error. With a systematic approach, you structure your evaluations to assess the performance of each component of the agent, as well as its end-to-end performance. For each component, you select the appropriate evaluators, testing examples, and metrics. This process helps you identify any areas of improvement so you can iterate on your agent during development and in production. In this course, you’ll build an AI agent, add observability to visualize and debug its steps, and evaluate its performance component-wise.",
    "url": "https://www.deeplearning.ai/short-courses/evaluating-ai-agents",
    "title": "Evaluating AI Agents",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn to evaluate, troubleshoot, and improve AI agents effectively—both during development and in production. Familiarity with prompting an LLM model would be helpful but not required.",
    "learningGoals": [
      "Learn how to add observability to your agent to gain insights into its steps and know how to debug it.",
      "Learn how to set up evaluations for the agent components by preparing testing examples, choosing the appropriate evaluator (code-based or LLM-as-a-Judge), and identifying the right metrics.",
      "Learn how to structure your evaluation into experiments to iterate on and improve the output quality and the path taken by your agent."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "John Gilhuly",
        "title": "Head of Developer Relations at Arize AI"
      },
      {
        "name": "Aman Khan",
        "title": "Director of Product at Arize AI"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Evaluation in the time of LLMs",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Decomposing agents",
        "usesCodeExample": false
      },
      {
        "duration": 16,
        "title": "Lab 1: Building your agent",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Tracing agents",
        "usesCodeExample": false
      },
      {
        "duration": 16,
        "title": "Lab 2: Tracing your agent",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Adding router and skill evaluations",
        "usesCodeExample": false
      },
      {
        "duration": 17,
        "title": "Lab 3: Adding router and skill evaluations",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Adding trajectory evaluations",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Lab 4: Adding trajectory evaluations",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Adding structure to your evaluations",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Lab 5: Adding structure to your evaluations",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Improving your LLM-as-a-judge",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Monitoring agents",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Resources, Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 66,
    "description": "This course clearly explains the ideas behind the attention mechanism. It walks through the algorithm itself and how to code it in PyTorch. Attention in Transformers: Concepts and Code in PyTorch, was built in collaboration with StatQuest, and taught by its Founder and CEO, Josh Starmer. The attention mechanism was a breakthrough that led to transformers, the architecture powering large language models like ChatGPT. Transformers, introduced in the 2017 paper “Attention is All You Need” by Ashish Vaswani and others, revolutionized AI with their scalable design. Learn how this foundational architecture works to improve your intuition about building reliable, functional, and scalable AI applications. What you’ll do: Understand the evolution of the attention mechanism, a key breakthrough that led to transformers. Learn the relationships between word embeddings, positional embeddings, and attention. Learn about the Query, Key, and Value matrices, how to produce them, and how to use them in attention. Go through the math required to calculate self-attention and masked self-attention to learn how and why the equation works the way it does. Understand the difference between self-attention and masked self-attention, and how one is used in the encoder to build context-aware embeddings and the other is used in the decoder for generative outputs. Learn the details of the encoder-decoder architecture, cross-attention, and multi-head attention, and how they are incorporated into a transformer. Use PyTorch to code a class that implements self-attention, masked self-attention, and multi-head attention.",
    "url": "https://www.deeplearning.ai/short-courses/attention-in-transformers-concepts-and-code-in-pytorch",
    "title": "Attention in Transformers: Concepts and Code in PyTorch",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn how the attention mechanism in LLMs like ChatGPT works.",
    "learningGoals": [
      "Learn how the attention mechanism in LLMs helps convert base token embeddings into rich context-aware embeddings.",
      "Understand the Query, Key, and Value matrices, what they are for, how to produce them, and how to use them in attention.",
      "Learn the difference between self-attention, masked self-attention, and cross-attention, and how multi-head attention scales the algorithm."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Josh Starmer",
        "title": "Founder and CEO of StatQuest"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "The Main Ideas Behind Transformers and Attention",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "The Matrix Math for Calculating Self-Attention",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Coding Self-Attention in PyTorch",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Self-Attention vs Masked Self-Attention",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "The Matrix Math for Calculating Masked Self-Attention",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Coding Masked Self-Attention in PyTorch",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Encoder-Decoder Attention",
        "usesCodeExample": false
      },
      {
        "duration": 2,
        "title": "Multi-Head Attention",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Coding Encoder-Decoder Attention and Multi-Head Attention in PyTorch",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 94,
    "description": "Introducing “How Transformer LLMs Work,” created with Jay Alammar and Maarten Grootendorst, authors of the “Hands-On Large Language Models” book. This course offers a deep dive into the main components of the transformer architecture that powers large language models (LLMs). The course covers the evolution of language representation, tokenization, transformer blocks, attention mechanisms, recent improvements, and implementations in the Hugging Face transformer library. By the end, learners will understand how LLMs process language and be able to comprehend related research papers, aiding in building LLM applications.",
    "url": "https://www.deeplearning.ai/short-courses/how-transformer-llms-work",
    "title": "How Transformer LLMs Work",
    "studentProfile": "Anyone interested in understanding the inner workings of transformer architectures that power today’s LLMs.",
    "learningGoals": [
      "Gain an understanding of the key components of transformers, including tokenization, embeddings, self-attention, and transformer blocks, to build a strong technical foundation.",
      "Understand recent transformer improvements to the attention mechanism such as KV cache, multi-query attention, grouped query attention, and sparse attention.",
      "Compare tokenization strategies used in modern LLMS and explore transformers in the Hugging Face Transformers library."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Jay Alammar",
        "title": "Director and Engineering Fellow at Cohere and co-author of Hands-On Large Language Models"
      },
      {
        "name": "Maarten Grootendorst",
        "title": "Senior Clinical Data Scientist at Netherlands Comprehensive Cancer Organization and co-author of Hands-On Large Language Models"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Understanding Language Models: Laguage as a Bag-of-Words",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Understanding Language Models: (Word) Embeddings",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Understanding Language Models: Encoding and Decoding Context with Attention",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Understanding Language Models: Transformers",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Tokenizers",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Architectural Overview",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "The Transformer Block",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Self-Attention",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Model Example",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Recent Improvements",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Mixture of Experts (MoE)",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips, Help, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 97,
    "description": "Building Towards Computer Use with Anthropic introduces an innovative capability from Anthropic that enables models to interact with and navigate computer interfaces. Taught by Colt Steele, Anthropic’s Head of Curriculum, this course covers Anthropic’s family of models and the building blocks that lead to the amazing new application – Computer Use. Computer Use utilizes the capabilities of the latest models including image reasoning and tool use to enable an LLM-based agent to use a computer. Like a human user, the model processes an image of the screen, analyzes it to understand what’s going on, and navigates the computer by issuing mouse clicks and generating keyboard strokes to get things done. In this course, you’ll learn the features that lead up to computer use from working with the Anthropic’s API, to multimodal prompting, prompt caching, and tool use, ending in a demo that combines these features to build an AI assistant that uses a computer.",
    "url": "https://www.deeplearning.ai/short-courses/building-towards-computer-use-with-anthropic",
    "title": "Building Towards Computer Use with Anthropic",
    "studentProfile": "Anyone who has basic Python knowledge, wants to learn how to use all of the features of the Anthropic family of models, and understand the capabilities of computer use applications.",
    "learningGoals": [
      "Learn about Anthropic’s family of models, its approach to AI research, and the best way to prompt it, including multi-modal use cases.",
      "Learn effective prompting techniques, implement prompt caching to reduce costs and latency, and build AI applications that can call tools.",
      "See how you can combine multimodal capabilities, agentic workflows, and tool use to build an AI assistant that can navigate and interact with computer interfaces, executing tasks like web searches."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Colt Steele",
        "title": "Head of Curriculum at Anthropic"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Overview",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Working with the API",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Multimodal Requests",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Real World Prompting",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Prompt Caching",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Tool Use",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Computer Use",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 64,
    "description": "Learn to use the Jamba model, a hybrid transformer-Mamba architecture trained to handle long contexts, in this new course, Build Long-Context AI Apps with Jamba, built in partnership with AI21 Labs and taught by Chen Wang and Chen Almagor. The transformer architecture is the foundation of most large language models but it is computationally expensive when handling very long input contexts. There’s an alternative to transformers called Mamba. Mamba is a selective state space model that can process very long contexts with a much lower computational cost. However, researchers found that the pure Mamba architecture underperforms in understanding the context, and the quality of the output can be lower even in tasks as simple as repeating the input in the output of the model. To overcome these challenges, AI21 Labs developed the Jamba model, which combines Mamba’s computational efficiency with the transformer’s attention mechanism to help with the output quality. In this course, you’ll learn about the Jamba architecture, how it works, and how it is trained. You’ll also learn how to prompt Jamba and use it to process long documents and build long-context RAG apps. In detail, you’ll learn key information about Jamba’s large context model and its unique, hybrid architecture that allows for high-quality outputs, high throughput, and low memory usage. Learn how Jamba combines transformers with Mamba, a selective state-space model to achieve high performance and quality. Use the AI21SDK, especially its document parameter, with an example of prompting a large over 200k-token annual report. Use Jamba for tool-calling with hands-on examples from calling simple arithmetic functions to a function that returns SEC 10-Q company quarterly report. Learn how the training for long context is done, and the metrics used for evaluating the performance. Create a RAG app using the AI21 Conversational RAG tool and build your own RAG pipeline that uses the Jamba model with LangChain. Start building AI apps that can handle context as long as all of your unread emails from the last 20 years!",
    "url": "https://www.deeplearning.ai/short-courses/build-long-context-ai-apps-with-jamba",
    "title": "Build Long-Context AI Apps with Jamba",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn more about how the Jamba model works and how it is used for building long-context AI apps.",
    "learningGoals": [
      "Learn how the Jamba model integrates transformers and the Mamba architecture to efficiently process long contexts while maintaining quality.",
      "Understand the training process for long context models, and the metrics used to evaluate their performance.",
      "Gain hands-on experience applying Jamba to tasks such as processing large documents, tool-calling, and building large context RAG apps."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Chen Wang",
        "title": "Lead Alliance Solution Architect at AI21 labs"
      },
      {
        "name": "Chen Almagor",
        "title": "Algorithm Team Lead at AI21 labs"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Overview",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Transformer-Mamba Hybrid LLM Architecture",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Jamba Prompting and Documents",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Tool Calling",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Expand the Context Window Size",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "Long Context Prompting",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Conversational RAG",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 84,
    "description": "Learn how to effectively prompt and use OpenAI’s o1 model in Reasoning with o1, a short course built in collaboration with OpenAI and taught by Colin Jarvis, Head of AI Solutions at OpenAI. The o1 model is exceptionally good at abstract reasoning tasks with record-breaking performance on planning, coding, analyzing, and domain-specific reasoning tasks. The course teaches reinforcement learning, chain-of-thought prompting, multi-step task implementation, coding competitions with models, image understanding, and meta-prompting techniques.",
    "url": "https://www.deeplearning.ai/short-courses/reasoning-with-o1",
    "title": "Reasoning with o1",
    "studentProfile": "It’s helpful to be familiar with Python and have a basic understanding of LLM prompting and LLM application development.",
    "learningGoals": [
      "Learn about o1, what makes it work, how it performs, and the best scenarios to use it.",
      "Learn how to prompt o1 effectively and when to delegate tasks to more cost-efficient, lower latency models.",
      "Learn how o1 outperforms on coding and vision reasoning tasks, and how to apply meta-prompting to optimize your applications."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Colin Jarvis",
        "title": "Head of AI Solutions at OpenAI"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Introduction to o1",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Prompting o1",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Planning with o1",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Coding with o1",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Reasoning with images",
        "usesCodeExample": true
      },
      {
        "duration": 26,
        "title": "Meta-prompting",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips, Help, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 37,
    "description": "Explore a new way to write and code with OpenAI Canvas, a user-friendly interface that allows you to brainstorm, draft, and refine text and code in collaboration with ChatGPT. In the short course Collaborative Writing and Coding with OpenAI Canvas, taught by Karina Nguyen, research lead at OpenAI, you’ll learn how to use this tool to enhance your writing and coding. Canvas provides a side-by-side workspace where you and ChatGPT can collaboratively edit and refine text or code. This course teaches to use the new interactive Canvas workspace and tools to make writing more flexible, efficient, and fun, with practical use cases like creating games, generating code from images, and building SQL databases. It also covers how GPT-4o powers Canvas's features.",
    "url": "https://www.deeplearning.ai/short-courses/collaborative-writing-and-coding-with-openai-canvas",
    "title": "Collaborative Writing and Coding with OpenAI Canvas",
    "studentProfile": "Anyone who wants to learn how to use the new OpenAI Canvas interface to write and code better.",
    "learningGoals": [
      "Learn to write and code more effectively with OpenAI Canvas, using features like targeted editing, in-line feedback, changing the reading level, final polish, reviewing code, and fixing bugs.",
      "Learn how to build practical use cases like creating a game app, generating Python code from plot screenshots, and creating SQL databases from architecture images.",
      "Gain knowledge of how GPT-4o was trained to bring the OpenAI Canvas features and functionalities to life."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Karina Nguyen",
        "title": "Research Lead at OpenAI"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Collaborative Writing",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Collaborative Coding",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Build a Spaceship Game",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Create SQL Database from Architecture Image",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Training the Canvas",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Free Canvas Access and Prompt Copies",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 56,
    "description": "Join our new short course, Building an AI-Powered Game, with Together AI and AI Dungeon! Learn from Niki Birkner, Senior Product Manager at Together AI, and Nick Walton, CEO and Co-Founder of Latitude. This course teaches you how to build a LLM-based application by creating your own AI-powered text-based game. You’ll create a world with hierarchical content generation based on a prompt, setting up its core mechanics, and giving it a simple Gradio user interface. By the end of this course, you’ll have a playable text-based game to share with your friends, built from scratch using LLMs. You will be guided through the building blocks of app development using LLMs, including incorporating safeguards using Llama Guard. What you’ll do: Use prompt engineering to create a world with hierarchical content generation so your narrative is consistent and the AI can flesh out your game. Build the first version of your game with a user interface, load the world, and define the core action loop of the game, so you can have a game to play and interact with. Implement safe user-dependent and context-dependent guardrails into your game application with Llama Guard and custom content policy. Learn LLM tool calling by adding story and state components that improve your game’s memory, track inventory, and show progress. With these techniques, you’ll hone your AI skills by building your own sharable game!",
    "url": "https://www.deeplearning.ai/short-courses/building-an-ai-powered-game",
    "title": "Building an AI-Powered Game",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn how to build with LLMs in a hands-on, fun way!",
    "learningGoals": [
      "Learn LLM app development by designing and developing a text-based AI game, creating immersive game worlds, characters, and storylines through hierarchical content generation.",
      "Learn to implement game mechanics using AI to convert text data into structured JSON outputs, enabling features like an inventory detection system.",
      "Learn to enforce safety and compliance for AI content creation, and create custom policies using Llama Guard."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Niki Birkner",
        "title": "Product Manager at Together AI"
      },
      {
        "name": "Nick Walton",
        "title": "CEO and Co-Founder of Latitude"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Hierarchical Content Generation",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Interactive AI Applications",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Moderation & Safety",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Implementing Game Mechanics",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips, Help, and Download",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 92,
    "description": "Join our new short course, Safe and Reliable AI via Guardrails, and learn how to build production-ready applications with Shreya Rajpal, co-founder & CEO of GuardrailsAI. The output of LLMs is fundamentally probabilistic, making it impossible to know in advance, or to guarantee the same response twice. This makes it difficult to put LLM-powered applications into production for industries with strict regulations or clients who require high consistency in application behavior. Installing guardrails on your system gives you an additional layer of control in creating safe and reliable applications. Guardrails act as safety mechanisms and validation tools built into AI applications, preventing your app from revealing incorrect, irrelevant, or sensitive information. This course teaches building robust guardrails from scratch to mitigate failure modes like hallucinations or revealing personally identifiable information (PII). You'll implement these in a RAG-powered customer service chatbot scenario for a small pizzeria, covering topics such as hallucination detection via Natural Language Inference, PII redaction, topic relevance enforcement, and competitor mention prevention.",
    "url": "https://www.deeplearning.ai/short-courses/safe-and-reliable-ai-via-guardrails",
    "title": "Safe and Reliable AI via Guardrails",
    "studentProfile": "Anyone who has basic Python knowledge looking to enhance the safety and reliability of LLM-powered applications with practical, hands-on guardrail techniques.",
    "learningGoals": [
      "Learn the common failure modes of LLM-powered applications that guardrails can help mitigate, including hallucinations and revealing sensitive information.",
      "Understand how AI guardrails validate and verify your applications with input and output guards, ensuring reliable and controlled interactions.",
      "Add guardrails to a RAG-powered customer service chatbot to create a new layer of control of the applications behavior."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Shreya Rajpal",
        "title": "Founder of GuardrailsAI"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "Failure modes in RAG applications",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "What are guardrails",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Building your first guardrail",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Checking for hallucinations with Natural Language Inference",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Using hallucination guardrail in a chatbot",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Keeping a chatbot on topic",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Ensuring no personal identifiable information (PII) is leaked",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Preventing competitor mentions",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 159,
    "description": "Join our new short course, Practical Multi AI Agents and Advanced Use Cases with crewAI, taught by João Moura, Founder of CrewAI, and learn how to build and deploy agent-based apps for advanced real applications in the industry. In this course, you will build several practical apps like an automated project planning system, lead-scoring and engagement automation, support data analysis, and content creation at scale. Throughout this course, you’ll learn the main building blocks—tasks, agents, crews—that go into creating these multi-agent systems, and all the different things that make them work such as caching, memory, and guardrails. You will learn how to integrate your multi-agent application with internal and external systems; connect multiple agents in complex setups including parallel, sequential, and hybrid; create flows involving multiple crews working together; test and optimize your crew's performance; work with multiple LLMs in your system; and start a project from scratch preparing it for deployment. The course also includes a bonus interview with Jacob Wilson, CTO at PWC, about deploying agentic workflows in real industry use cases.",
    "url": "https://www.deeplearning.ai/short-courses/practical-multi-ai-agents-and-advanced-use-cases-with-crewai",
    "title": "Practical Multi AI Agents and Advanced Use Cases with crewAI",
    "studentProfile": "Anyone who has basic Python knowledge and wants to build complex and practical multi-agentic systems.",
    "learningGoals": [
      "Build agents that collaborate in complex workflows, integrating external tools and using different models to handle specific tasks efficiently.",
      "Learn to evaluate and enhance your agents by conducting performance tests and providing human feedback to train and optimize their performance.",
      "Create multi-agent systems to automate tasks, such as project planning, lead scoring, data reporting, and large-scale content creation."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "João Moura",
        "title": "Founder and CEO of CrewAI"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Overview of Multi AI-Agent Systems",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Automated Project: Planning, Estimation, and Allocation",
        "usesCodeExample": true
      },
      {
        "duration": 3,
        "title": "Internal and External Integrations",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Building Project Progress Report",
        "usesCodeExample": true
      },
      {
        "duration": 3,
        "title": "Complex crew Setups",
        "usesCodeExample": false
      },
      {
        "duration": 28,
        "title": "Agentic Sales Pipeline",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Performance Optimization",
        "usesCodeExample": false
      },
      {
        "duration": 25,
        "title": "Support Data Insight Analysis",
        "usesCodeExample": true
      },
      {
        "duration": 3,
        "title": "Multi-Model Use Cases",
        "usesCodeExample": false
      },
      {
        "duration": 17,
        "title": "Content Creation at Scale",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Agentic Workflows in Industry",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Generate, Deploy and Monitor Crews",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Blog Post Crew in Production",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 94,
    "description": "Agentic workflows handle unpredictable tasks based on user input, like making API calls. A serverless architecture efficiently manages these tasks and varying workloads without maintaining servers, enabling faster deployment.\n\nYou will learn to protect sensitive information and shield customers from harmful content by employing agents with guardrails.\n\nThis course teaches you to build and deploy a serverless agentic application. You’ll learn to create agents with tools, code execution, and guardrails. The serverless setup is ideal for agents that might need to access many tools or APIs on demand.\n\nYou’ll explore this through hands-on examples where you’ll:  \n- Build a customer service bot for a fictional tea mug business that can handle tasks like answering queries, retrieving information, and processing orders.\n- Connect multiple types of agent actions, and implement guardrails for responsible operation.\n- Use Amazon Bedrock’s fully managed services to deploy and scale the bot efficiently.\n\nThe course will implement two elements essential to the deployment of business applications:\n1. Serverless deployment to achieve rapid scaling and seamless operation without the need to manage infrastructure.\n2. Responsible agent to protect your application from malicious prompts and unintended outputs by configuring guardrails.\n\nIn detail, here’s what you’ll do:  \n- Use Amazon Bedrock to create an AI agent, explore how you invoke the agent, and see the trace to review the agent’s thought process and observation loop until it reaches its final output.\n- Connect your customer service agent to services like a CRM to get customer details and log support tickets in real time.\n- Attach a code interpreter to your agent, giving it the ability to perform accurate calculations, where it writes and runs its own Python code to support its response.\n- Implement and configure guardrails to prevent your agent from revealing sensitive information and using inappropriate language.\n- Connect your agent to a repository of customer support documents that discuss many issues that the agent can resolve directly or choose to escalate, if necessary, to a human workflow.\n- Get a walkthrough of the Amazon Bedrock interface in the AWS console to configure agents, set the guardrails, and connect to knowledge databases, all in an easy-to-configure graphical interface.\n\nBy the end, you will have built a sophisticated AI agent capable of handling real-world customer support scenarios, fully serverless, and ready to scale.",
    "url": "https://www.deeplearning.ai/short-courses/serverless-agentic-workflows-with-amazon-bedrock",
    "title": "Serverless Agentic Workflows with Amazon Bedrock",
    "studentProfile": "Anyone who has basic Python knowledge and wants to build an agentic application for business or personal use. It’s useful to understand serverless deployment and cloud-based applications.",
    "learningGoals": [
      "Build and deploy scalable serverless agentic applications with Amazon Bedrock.",
      "Integrate tools, code execution, and guardrails to manage agentic actions effectively.",
      "Design responsible agents with safeguards to prevent malicious prompts and unintended outputs."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Mike Chambers",
        "title": "Developer Advocate for Generative AI at AWS, Co-instructor of Generative AI with Large Language Models"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Your first agent with Amazon Bedrock",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Connecting with a CRM",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Performing calculations",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Guard Rails",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Reading the FAQ manual",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Console Walkthrough",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 79,
    "description": "Join our new short course, Introducing Multimodal Llama 3.2, and learn from Amit Sangani, Senior Director of AI Partner Engineering at Meta, to learn all about the latest additions to the Llama models 3.1 and 3.2, from custom tool calling to multimodality and the new Llama stack. Open models are a key building block of AI and a key enabler of AI research. With Meta’s family of open models, anyone can download, customize, fine-tune, or build new applications on top of them, allowing AI innovation. The Llama model family now ranges from 1B model parameters to its 405B foundation model, allowing for diverse use cases and applications. In this course, you’ll learn about the new vision capabilities that Llama 3.2 brings to the Llama family. You’ll learn how to leverage this along with tool-calling, and Llama Stack, which is an open-source orchestration layer for building on top of the Llama family of models. In detail, you’ll learn about the new models, how they were trained, their features, and how they fit into the Llama family. Understand how to do multimodal prompting with Llama and work on advanced image reasoning use cases such as understanding errors on a car dashboard, adding up the total of three restaurant receipts, grading written math homework, and many more. Learn different roles—system, user, assistant, ipython—in the Llama 3.1 and 3.2 family and the prompt format that identifies those roles. Understand how Llama uses the tiktoken tokenizer, and how it has expanded to a 128k vocabulary size that improves encoding efficiency and enables support for seven non-English languages. Learn how to prompt Llama to call both built-in and custom tools with examples for web search and solving math equations. Learn about ‘Llama Stack API’, which is a standardized interface for canonical toolchain components like fine-tuning or synthetic data generation to customize Llama models and build agentic applications.",
    "url": "https://www.deeplearning.ai/short-courses/introducing-multimodal-llama-3-2",
    "title": "Introducing Multimodal Llama 3.2",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn to quickly build on Llama and Llama stack.",
    "learningGoals": [
      "Explore the features of the new Llama 3.2 model, from image classification, vision reasoning to tool use.",
      "Learn the details of Llama 3.2 prompting, tokenization, built-in and custom tool calling.",
      "Gain knowledge of the Llama stack, which is a standardized interface for building AI applications."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Amit Sangani",
        "title": "Senior Director of Partner Engineering of Meta"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Overview of Llama 3.2",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Multimodal Prompting",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Multimodal Use Cases",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Prompt Format",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Tokenization",
        "usesCodeExample": true
      },
      {
        "duration": 18,
        "title": "Tool Calling",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Llama Stack",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 93,
    "description": "In Retrieval Optimization: From Tokenization to Vector Quantization, taught by Kacper Łukawski, Developer Relations Lead of Qdrant, you'll learn all about tokenization and also how to optimize vector search in your large-scale customer-facing RAG applications. This course focuses on optimizing the first step in your RAG and search results. You'll see how different tokenization techniques like Byte-Pair Encoding, WordPiece, and Unigram work and how they affect search relevancy. You'll also learn how to address common challenges such as terminology mismatches and truncated chunks in embedding models. To optimize your search, you need to be able to measure its quality. You will learn several quality metrics for this purpose. Most vector databases use Hierarchical Navigable Small Worlds (HNSW) for approximate nearest-neighbor search. You'll see how to balance the HNSW parameters for higher speed and maximum relevance. Finally, you would use different vector quantization techniques to enhance memory usage and search speed.",
    "url": "https://www.deeplearning.ai/short-courses/retrieval-optimization-from-tokenization-to-vector-quantization",
    "title": "Retrieval Optimization: From Tokenization to Vector Quantization",
    "studentProfile": "Anyone with basic Python knowledge who wants to learn to build effective customer-facing RAG applications!",
    "learningGoals": [
      "Learn how tokenization works in large language and embedding models and how the tokenizer can affect the quality of your search.",
      "Explore how different tokenization techniques including Byte-Pair Encoding, WordPiece, and Unigram are trained and work.",
      "Understand how to measure the quality of your retrieval and how to optimize your search by adjusting HNSW parameters and vector quantizations."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Kacper Łukawski",
        "title": "Developer Relations Lead at Qdrant"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 16,
        "title": "Embedding models",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Role of the tokenizers",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Practical implications of the tokenization",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Measuring Search Relevance",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Optimizing HNSW search",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Vector quantization",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 60,
    "description": "This course offers intermediate-level content focused on multimodal retrieval-augmented generation (RAG) techniques, enabling chat interactions with videos. It includes 8 video lessons and 6 code examples, taught by Vasudev Lal, a Principal AI Research Scientist at Intel Labs.",
    "url": "https://www.deeplearning.ai/short-courses/multimodal-rag-chat-with-videos",
    "title": "Multimodal RAG: Chat with Videos",
    "studentProfile": "The course is intended for learners with an intermediate understanding of AI and multimodal systems who want to explore retrieval-augmented generation applied to videos.",
    "learningGoals": [
      "Understand Multimodal Retrieval-Augmented Generation (RAG) techniques.",
      "Learn to chat with videos using AI models.",
      "Gain knowledge on integrating video data in AI applications.",
      "Explore intermediate-level concepts with practical examples in 1 hour.",
      "Develop skills through 8 video lessons and 6 code examples."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Vasudev Lal",
        "title": "Principal AI Research Scientist at Intel Labs"
      }
    ],
    "courseItems": [],
    "usesCodeExamples": false
  },
  {
    "duration": 615,
    "description": "AI Python for Beginners is designed to help you leverage the power of Python programming, even if your goal isn’t to become a software developer or AI engineer. This four-part course teaches you to code practical AI applications from day one, whether you’re an experienced programmer, or writing “Hello, World!” for the first time. You’ll learn with support from an AI chatbot that can provide you with immediate feedback, answer your questions, quickly identify and work through bugs, and keep you on track while learning new skills. You’ll gain a foundational understanding of Python while using it to build AI-powered tools like custom recipe generators, smart to-do lists, and vacation planners, learning essential programming concepts such as variables, functions, loops, and data structures along the way. By the end of this course series, you’ll be able to write Python scripts that interact with large language models, automate tasks, and analyze your own data. You’ll even learn how to extend Python’s capabilities using popular third-party packages for data analysis and visualization, and how to access real-time information through APIs. These are skills that are increasingly valuable across industries from tech and finance to healthcare and creative fields.",
    "url": "https://www.deeplearning.ai/short-courses/ai-python-for-beginners",
    "title": "AI Python for Beginners",
    "studentProfile": "This course is for anyone curious about AI and programming with Python, from complete beginners learning to code for the first time to professionals seeking to boost productivity and learn how to properly integrate AI into their coding process. Ideal for students, career changers, knowledge workers, lifelong learners, and educators. If traditional coding courses haven’t worked or have felt intimidating, our hands-on, AI-focused approach will help you in your journey. Whether you want to automate repetitive tasks, extract insights from large datasets, or create AI-powered tools to enhance your work or personal projects, this course will give you the skills to get started. By the end, you’ll not only understand Python basics but also know how to leverage powerful data analysis libraries, interact with web APIs, and set up Python on your own computer to continue your learning journey.",
    "learningGoals": [
      "Learn Python programming fundamentals and how to integrate AI tools for data manipulation, analysis, and visualization.",
      "Discover how Python can be applied in various domains such as business, marketing, and journalism to solve real-world problems and enhance efficiency through practical applications.",
      "Leverage AI assistants to debug code, explain concepts, and enhance your learning, mirroring real-world software development practices."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Andrew Ng",
        "title": "Founder, DeepLearning.AI; Co-founder, Coursera"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "What is computer programming?",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Writing code with chatbots",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "Navigating the learning platform",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Running your first program",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "How to succeed in coding",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Data in Python",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Combining text and calculations",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Variables",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Building LLM prompts with variables",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Functions: Actions on Data",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Quiz 1",
        "usesCodeExample": false
      },
      {
        "duration": 90,
        "title": "Working with a Virtual Library",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 118,
    "description": "Multimodal models like Gemini are pushing the boundaries of what’s possible by unifying traditionally siloed data modalities. With Gemini, you can build applications that seamlessly understand and reason across text, images, and videos, enabling a new class of intelligent systems. For example, building a virtual interior designer that can analyze a user’s room images, understand their style preferences from a text description, and generate personalized design recommendations. Or creating a smart document processing pipeline that can extract structured data from complex PDFs, answer questions based on the content, and generate human-like summaries. You’ll learn prompt engineering techniques to guide Gemini’s behavior and optimize its performance for diverse use cases, from creative story generation to analytical report writing. And you’ll discover how to integrate Gemini with external APIs and databases using function calling, with the ability to infuse your applications with real-time data and dynamic content.",
    "url": "https://www.deeplearning.ai/short-courses/large-multimodal-model-prompting-with-gemini",
    "title": "Large Multimodal Model Prompting with Gemini",
    "studentProfile": "Whether your goal is to build next-gen document understanding systems, intelligent video search tools, or interactive virtual assistants, this course will equip you with the skills to develop transformative applications.",
    "learningGoals": [
      "Learn state-of-the-art techniques for getting the most out of multimodal AI with Google’s Gemini model family.",
      "Leverage the power of Gemini’s cross-modal attention to fuse information from text, images, and video for complex reasoning tasks.",
      "Extend Gemini’s capabilities with external knowledge and live data via function calling and API integration."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Erwin Huizenga",
        "title": "Developer Advocate for Generative AI on Google Cloud"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Introduction to Gemini Models",
        "usesCodeExample": false
      },
      {
        "duration": 26,
        "title": "Multimodal Prompting and Parameter Control",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Best Practices for Multimodal Prompting",
        "usesCodeExample": false
      },
      {
        "duration": 20,
        "title": "Creating Use Cases with Images",
        "usesCodeExample": false
      },
      {
        "duration": 26,
        "title": "Developing Use Cases with Videos",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Integrating Real-Time Data with Function Calling",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": false
  },
  {
    "duration": 85,
    "description": "In Building AI Applications with Haystack you will learn a high-level orchestration framework that helps ensure your applications are flexible, extendible, and maintainable, even as the technology stack changes, user needs arise, and new features are added. Using a framework can provide common features out of the box that significantly speeds up the development process. Haystack offers robust and flexible architecture and framework for building AI applications. It manages complexity and helps you focus more on developing your application at a higher level of abstraction. Throughout the course, you will develop several projects, including a RAG app, a news summarization app, a chat agent with function calling, and a self-reflecting agent with loops.",
    "url": "https://www.deeplearning.ai/short-courses/building-ai-applications-with-haystack",
    "title": "Building AI Applications with Haystack",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn to build and customize complex apps using Haystack!",
    "learningGoals": [
      "Learn how Haystack components, pipelines, and document stores can be used to build custom AI applications.",
      "Build a RAG pipeline and extend its ability by creating custom components.",
      "Use conditional branching, self-reflecting agents, and function calling to build complex pipelines and apps."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Tuana Çelik",
        "title": "Developer Relations Lead at Haystack by Deepset"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Haystack Building Blocks",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Build Customized RAG",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Custom Components -- News Summarizer",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Fallbacks with Branching Pipelines",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Self-Reflecting Agents with Loops",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Chat Agent with Function Calling",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 101,
    "description": "Join our new short course, Improving Accuracy of LLM Applications with Lamini and Meta. Learn from Sharon Zhou, Co-founder & CEO of Lamini, and Amit Sangani, Senior Director of Partner Engineering, Meta. Many developers have experienced frustration with inconsistent results when working with LLM applications. This course offers a systematic approach to enhance the accuracy and reliability of your LLM applications. You will build an SQL agent, add evaluation metrics to measure performance, and use prompt engineering and self-reflection to make the model perform better. Finally, you will fine-tune the model with techniques like LoRA and memory tuning that embeds facts in model weights to reduce hallucinations. In this course, you’ll use Llama’s family of open-source models. What you’ll do: Build a text to SQL agent and simulate situations where it hallucinates to begin the evaluation process. Build an evaluation framework to systematically measure performance, including criteria for good evaluations, best practices, and how to develop an evaluation score. Learn how instruction fine-tuning enhances pre-trained LLMs to follow instructions, and how memory fine-tuning embeds facts to reduce hallucinations. Break fine-tuning myths and see how Performance-Efficient Fine-tuning (PEFT) techniques like Low-Rank Adaptation(LoRA) reduce training time by 100x and Mixture of Memory Experts (MoME) reduces it even further. Go through an iterative process of generating training data and fine-tuning, learning practical tips such as adding examples, generating variations, and filtering generated data to increase model accuracy.",
    "url": "https://www.deeplearning.ai/short-courses/improving-accuracy-of-llm-applications",
    "title": "Improving Accuracy of LLM Applications",
    "studentProfile": "This course is ideal for anyone with intermediate Python knowledge and familiarity with large language models (LLMs) looking to build more factual and precise LLM applications.",
    "learningGoals": [
      "Understand development steps, from evaluation, through prompting, self-reflection, and fine-tuning, to improve your model’s reliability and accuracy.",
      "Learn how memory tuning can increase your model performance by embedding facts into your model to reduce hallucination.",
      "Use the Llama 3-8b model to build an LLM application that converts text to SQL with a custom schema."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Sharon Zhou",
        "title": "Co-Founder and CEO of Lamini"
      },
      {
        "name": "Amit Sangani",
        "title": "Senior Director of Partner Engineering of Meta"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 17,
        "title": "Overview",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Create an SQL Agent",
        "usesCodeExample": true
      },
      {
        "duration": 23,
        "title": "Create an Evaluation",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Finetuning, PEFT, & Memory Tuning",
        "usesCodeExample": false
      },
      {
        "duration": 31,
        "title": "Generate Data & Finetune",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 50,
    "description": "Join our new short course, Embedding Models: From Architecture to Implementation! Learn from Ofer Mendelevitch, Head of Developer Relations at Vectara. This course goes into the details of the architecture and capabilities of embedding models, which are used in many AI applications to capture the meaning of words and sentences. You will learn about the evolution of embedding models, from word to sentence embeddings, and build and train a simple dual encoder model. This hands-on approach will help you understand the technical concepts behind embedding models and how to use them effectively. In detail, you’ll: Learn about word embedding, sentence embedding, and cross-encoder models; and how they can be used in RAG. Understand how transformer models, specifically BERT (Bi-directional Encoder Representations from Transformers), are trained and used in semantic search systems. Gain knowledge of the evolution of sentence embedding and understand how the dual encoder architecture was formed. Use a contrastive loss to train a dual encoder model, with one encoder trained for questions and another for the responses. Utilize separate encoders for question and answer in a RAG pipeline and see how it affects the retrieval compared to using a single encoder model. By the end of this course, you will understand word, sentence, and cross-encoder embedding models, and how transformer-based models like BERT are trained and used in semantic search. You will also learn how to train dual encoder models with contrastive loss and evaluate their impact on retrieval in a RAG pipeline.",
    "url": "https://www.deeplearning.ai/short-courses/embedding-models-from-architecture-to-implementation",
    "title": "Embedding Models: From Architecture to Implementation",
    "studentProfile": "This course is ideal for data scientists, machine learning engineers, NLP enthusiasts, and anyone who wants to learn about the creation and implementation of embedding models, which are crucial for building semantic retrieval systems. Whether you’re familiar with generative AI applications or new to the concept, if you have basic Python knowledge, this course offers a deep dive into how these models are built and capture the semantic meaning of words and sentences.",
    "learningGoals": [
      "Gain an in-depth understanding of the architecture behind embedding models; and learn how to train and use them.",
      "Learn how to use different embedding models such as Word2Vec and BERT in various semantic search systems.",
      "Learn how to build and train dual encoder models using contrastive loss, enhancing the accuracy of question-answer retrieval applications."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Ofer Mendelevitch",
        "title": "Head of Developer Relations at Vectara"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Introduction to embedding models",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Contextualized token embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Token vs. sentence embedding",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Training a dual encoder",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Using embeddings in RAG",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix – Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 68,
    "description": "Join Federated Learning! In this two-part course series, you will use Flower, a popular open source framework, to build a federated learning system, and learn about federated fine-tuning of LLMs with private data in part two. Federated learning allows models to be trained across multiple devices or organizations without sharing data, improving privacy and security. Federated learning also has many practical uses, such as training next-word prediction models on mobile keyboards without transmitting sensitive keystrokes onto a central server. First, you’ll learn about the federated training process, how to tune and customize it, how to increase data privacy, and how to manage bandwidth usage in federated learning. Then, you’ll learn to apply federated learning to LLMs. You’ll explore challenges like data memorization and the computational resources required by LLMs, and explore techniques for efficiency and privacy enhancement, such as Parameter-Efficient Fine-Tuning (PEFT) and Differential Privacy (DP). This two-part course series is self-contained. If you already know what federated learning is, you can start directly with part two of the course.",
    "url": "https://www.deeplearning.ai/short-courses/intro-to-federated-learning",
    "title": "Federated Learning",
    "studentProfile": "Anyone who has a basic background in Python and machine learning, has an understanding of LLMs, and wants to learn how to build models, including large language models, on private distributed data using the Flower framework.",
    "learningGoals": [
      "Explore the components of federated learning systems and learn to customize, tune, and orchestrate them for better model training.",
      "Leverage federated learning to enhance LLMs by effectively managing key privacy and efficiency challenges.",
      "Learn how techniques like parameter-efficient fine-tuning and differential privacy are crucial for making federated learning secure and efficient."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Daniel J. Beutel",
        "title": "Co-Founder & CEO of Flower Labs"
      },
      {
        "name": "Nicholas Lane",
        "title": "Co-founder and Chief Scientific Officer of Flower Labs"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Why Federated Learning",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Federated Training Process",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Tuning",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Data Privacy",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Bandwidth",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 79,
    "description": "In Pretraining LLMs you’ll explore the first step of training large language models using a technique called pretraining. You’ll learn the essential steps to pretrain an LLM, understand the associated costs, and discover how starting with smaller, existing open source models can be more cost-effective.\n\nPretraining involves teaching an LLM to predict the next token using vast text datasets, resulting in a base model, and this base model requires further fine-tuning for optimal performance and safety. In this course, you’ll learn to pretrain a model from scratch and also to take a model that’s already been pretrained and continue the pretraining process on your own data. \n\nIn detail:\n\n- Explore scenarios where pretraining is the optimal choice for model performance. Compare text generation across different versions of the same model to understand the performance differences between base, fine-tuned, and specialized pre-trained models.\n- Learn how to create a high-quality training dataset using web text and existing datasets, which is crucial for effective model pretraining.\n- Prepare your cleaned dataset for training. Learn how to package your training data for use with the Hugging Face library.\n- Explore ways to configure and initialize a model for training and see how these choices impact the speed of pretraining.\n- Learn how to configure and execute a training run, enabling you to train your own model.\n- Learn how to assess your trained model’s performance and explore common evaluation strategies for LLMs, including important benchmark tasks used to compare different models’ performance.\n\nAfter taking this course, you’ll be equipped with the skills to pretrain a model—from data preparation and model configuration to performance evaluation.",
    "url": "https://www.deeplearning.ai/short-courses/pretraining-llms",
    "title": "Pretraining LLMs",
    "studentProfile": "This course is ideal for AI enthusiasts, data scientists, and machine learning engineers who want to learn the complete process of pretraining LLMs. Basic knowledge of Python and large language models is recommended.",
    "learningGoals": [
      "Gain in-depth knowledge of the steps to pretrain an LLM, encompassing all the steps, from data preparation, to model configuration and performance assessment.",
      "Explore various options for configuring your model’s architecture, including modifying Meta’s Llama models to create larger or smaller versions and initializing weights either randomly or from other models.",
      "Learn innovative pretraining techniques like Depth Upscaling, which can reduce training costs by up to 70%."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Sung Kim",
        "title": "CEO of Upstage"
      },
      {
        "name": "Lucy Park",
        "title": "Chief Scientific Officer of Upstage"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Why Pre-training",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Data Preparation",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Packaging Data for Pretraining",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Model Initialization",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Training in Action",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Evaluation",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 99,
    "description": "This course focuses on integrating traditional database features with vector search capabilities to optimize the performance and cost-efficiency of large-scale Retrieval Augmented Generation (RAG) applications. You’ll learn how to apply key techniques including prefiltering and postfiltering for filtering results based on specific conditions, projection to minimize size output, reranking to reorder search results for relevance, and prompt compression to reduce prompt length for cost efficiency. The course includes hands-on exercises using MongoDB for vector search, multi-stage aggregation pipelines, metadata filtering, projection stages, reranking, and prompt compression in LLM applications.",
    "url": "https://www.deeplearning.ai/short-courses/prompt-compression-and-query-optimization",
    "title": "Prompt Compression and Query Optimization",
    "studentProfile": "This course is suitable for those who are familiar with Python and have a basic understanding of databases and vector search.",
    "learningGoals": [
      "Combine vector search capabilities with traditional database operations to build efficient and cost-effective RAG applications.",
      "Learn how to use pre-filtering, post-filtering, and projection techniques for faster query processing and optimized query output.",
      "Use prompt compression techniques to reduce the length of prompts that are expensive to process in large-scale applications."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Richmond Alake",
        "title": "Developer Advocate at MongoDB"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 33,
        "title": "Vanilla Vector Search",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "Filtering With Metadata",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Projections",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Boosting",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Prompt Compression",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix-Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 97,
    "description": "Learn how to perform model training and inference jobs with cleaner, low-carbon energy in the cloud! Learn from Nikita Namjoshi, developer advocate at Google Cloud and Google Fellow, and explore how to measure the environmental impact of your machine learning jobs, and optimize their use of clean electricity. Topics include querying real-time electricity grid data, training models with low-carbon energy, retrieving carbon footprint measurements, and using the Google Cloud Carbon Footprint tool. Throughout the course, you’ll work with ElectricityMaps API and Google Cloud to run model training jobs powered by low-carbon energy.",
    "url": "https://www.deeplearning.ai/short-courses/carbon-aware-computing-for-genai-developers",
    "title": "Carbon Aware Computing for GenAI Developers",
    "studentProfile": "Familiarity with Python will help with the coding parts of the lesson, although the concepts covered can help anyone gain an understanding of the environmental impact of machine learning workflows.",
    "learningGoals": [
      "Retrieve real-time data on global energy mixes and carbon intensity from the ElectricityMaps API. Identify power grids that produce electricity from low-carbon sources, such as hydro, nuclear, wind, and solar power.",
      "Run a machine learning training job using low-carbon electricity by re-directing training tasks to cloud server locations selected based on their average and real-time carbon intensity measurements.",
      "Analyze the carbon footprint of sample Google Cloud usage data, including machine learning training, inference, storage, and other API activities."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Nikita Namjoshi",
        "title": "Developer Advocate at Google Cloud"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "The Carbon Footprint of Machine Learning",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "Exploring Carbon Intensity on the Grid",
        "usesCodeExample": true
      },
      {
        "duration": 18,
        "title": "Training Models in Low Carbon Regions",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "Using Real-Time Energy Data for Low-Carbon Training",
        "usesCodeExample": true
      },
      {
        "duration": 20,
        "title": "Understanding your Google Cloud Footprint",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Next steps",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Google Cloud Setup",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 59,
    "description": "This course will teach you two critical skills for building applications with LLMs: function-calling and structured data extraction. Function-calling allows you to extend LLMs with custom capabilities by enabling them to form calls to external functions based on natural language instructions. Structured data extraction enables LLMs to pull usable information from unstructured text. You'll work with NexusRavenV2-13B, an open source model fine-tuned for function-calling and data extraction. The model, available on Hugging Face, has outperformed GPT-4 in some function-calling tasks, and has 13 billion parameters so it can be hosted locally. The skills you’ll learn in this course will allow you to build advanced AI agents and assistants that can process and analyze customer feedback, automate data entry and content management workflows, enhance search and recommendation systems with structured data, and many other real-world applications.",
    "url": "https://www.deeplearning.ai/short-courses/function-calling-and-data-extraction-with-llms",
    "title": "Function-Calling and Data Extraction with LLMs",
    "studentProfile": "This course is designed for anyone interested in using function-calling capabilities with large language models to interface with external tools and extract structured data. Familiarity with LLMs and basic Python knowledge are recommended.",
    "learningGoals": [
      "Learn to extend LLMs with custom functionality via function-calling, enabling them to form calls to external functions.",
      "Extract structured data from natural language inputs, making real-world data usable for analysis.",
      "Build an end-to-end application that processes customer service transcripts using LLMs."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Jiantao Jiao",
        "title": "Co-founder & CEO of Nexusflow and Assistant Professor of EECS and Statistics at UC Berkeley"
      },
      {
        "name": "Venkat Srinivasan",
        "title": "Founding Engineer at Nexusflow"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "What is function calling",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Function calling variations",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Interfacing with external tools",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Structured Extraction",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Applications",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Course project dialog processing",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 58,
    "description": "In Building Your Own Database Agent you will develop an AI agent that interacts with databases using natural language, simplifying the process for querying and extracting insights. Created in partnership with Microsoft and taught by Adrian Gonzalez Sanchez, Data and AI Specialist at Microsoft, this course is designed for developers, data professionals, as well as business analysts and professionals who want more sophisticated interaction with their databases through natural language instead of advanced SQL queries. By the end of the course, you’ll be equipped with the technical knowledge and practical experience to implement similar systems in your own projects or organizations, enabling more efficient and accessible data interaction and analysis.",
    "url": "https://www.deeplearning.ai/short-courses/building-your-own-database-agent",
    "title": "Building Your Own Database Agent",
    "studentProfile": "If you want to learn how to interact with databases through natural language, this beginner-friendly course is for you. Familiarity with Python programming and databases (CSV files and SQL) is recommended, but not required.",
    "learningGoals": [
      "Interact with tabular data and SQL databases using natural language, enabling more efficient and accessible data analysis.",
      "Gain hands-on experience with the Azure OpenAI Service, implementing techniques like Retrieval Augmented Generation (RAG) and function calling.",
      "Use Azure OpenAI Service’s Assistants API, and test it with function calling and code interpreter features."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Adrian Gonzalez Sanchez",
        "title": "Data & AI Specialist at Microsoft"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Your First AI Agent",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Interacting with a CSV Data",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Connecting to a SQL Database",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Azure OpenAI Function Calling Feature",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Leveraging Assistants API for SQL Databases",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 92,
    "description": "LangChain, a popular open source framework for building LLM applications, recently introduced LangGraph. This extension allows developers to create highly controllable agents. In this course you will learn to build an agent from scratch using Python and an LLM, and then you will rebuild it using LangGraph, learning about its components and how to combine them to build flow-based applications. Additionally, you will learn about agentic search, which returns multiple answers in an agent-friendly format, enhancing the agent’s built-in knowledge. This course will show you how to use agentic search in your applications to provide better data for agents to enhance their output. You will also learn to implement persistence for state management, incorporate human-in-the-loop into agent systems, and develop an agent for essay writing replicating the workflow of a researcher.",
    "url": "https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph",
    "title": "AI Agents in LangGraph",
    "studentProfile": "If you have intermediate Python knowledge and want to learn how to create more controllable agents using the LangGraph open source framework, this course is for you.",
    "learningGoals": [
      "Learn about LangGraph’s components and how they enable the development, debugging, and maintenance of AI agents.",
      "Integrate agentic search capabilities to enhance agent knowledge and performance.",
      "Learn directly from LangChain founder Harrison Chase and Tavily founder Rotem Weiss."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Harrison Chase",
        "title": "Co-Founder and CEO of LangChain"
      },
      {
        "name": "Rotem Weiss",
        "title": "Co-founder and CEO of Tavily"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Build an Agent from Scratch",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "LangGraph Components",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Agentic Search Tools",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Persistence and Streaming",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Human in the loop",
        "usesCodeExample": true
      },
      {
        "duration": 18,
        "title": "Essay Writer",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "LangChain Resources",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 85,
    "description": "In AI Agentic Design Patterns with AutoGen you’ll learn how to build and customize multi-agent systems, enabling agents to take on different roles and collaborate to accomplish complex tasks using AutoGen, a framework that enables development of LLM applications using multi-agents. The course includes hands-on projects such as multi-agent conversations, customer onboarding experiences, blog post writing with agent reflection, conversational chess game with tool use, coding for financial analysis, and multi-agent collaboration for stock report generation. By the end, learners gain practical skills to implement multi-agent systems effectively with AutoGen.",
    "url": "https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen",
    "title": "AI Agentic Design Patterns with AutoGen",
    "studentProfile": "If you have basic Python coding experience and you’re interested in automating complex workflows using AI agents, this course will provide the practical skills and knowledge you need to leverage AutoGen effectively.",
    "learningGoals": [
      "Use the AutoGen framework to build multi-agent systems with diverse roles and capabilities for implementing complex AI applications.",
      "Implement agentic design patterns: Reflection, Tool use, Planning, and Multi-agent collaboration using AutoGen.",
      "Learn directly from the creators of AutoGen, Chi Wang and Qingyun Wu."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Chi Wang",
        "title": "Principal Researcher at Microsoft Research"
      },
      {
        "name": "Qingyun Wu",
        "title": "Assistant Professor at Penn State University"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Multi-Agent Conversation and Stand-up Comedy",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Sequential Chats and Customer Onboarding",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Reflection and Blogpost Writing",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Tool Use and Conversational Chess",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Coding and Financial Analysis",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Planning and Stock Report Generation",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 59,
    "description": "As AI moves beyond the cloud, on-device inference is rapidly expanding to smartphones, IoT devices, robots, AR/VR headsets, and more. Billions of mobile and other edge devices are ready to run optimized AI models. This course equips you with key skills to deploy AI on device: Explore how deploying models on device reduces latency, enhances efficiency, and preserves privacy. Go through key concepts of on-device deployment such as neural network graph capture, on-device compilation, and hardware acceleration. Convert pretrained models from PyTorch and TensorFlow for on-device compatibility. Deploy a real-time image segmentation model on device with just a few lines of code. Test your model performance and validate numerical accuracy when deploying to on-device environments Quantize and make your model up to 4x faster and 4x smaller for higher on-device performance. See a demonstration of the steps for integrating the model into a functioning Android app.",
    "url": "https://www.deeplearning.ai/short-courses/introduction-to-on-device-ai",
    "title": "Introduction to On-Device AI",
    "studentProfile": "This course is designed for beginner AI developers, ML engineers, data scientists, and mobile developers looking to deploy optimized models on edge devices. Familiarity with Python, as well as PyTorch or TensorFlow is recommended.",
    "learningGoals": [
      "Learn to deploy AI models on edge devices like smartphones, using their local compute power for faster and more secure inference.",
      "Explore model conversion by, converting your PyTorch/TensorFlow models for device compatibility, and quantize them to achieve performance gains while reducing model size.",
      "Learn about device integration, including runtime dependencies, and how GPU, NPU, and CPU compute unit utilization affect performance."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Krishna Sridhar",
        "title": "Senior Director of Engineering at Qualcomm"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Why on-device",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Deploying Segmentation Models On-Device",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Preparing for on-device deployment",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Quantizing Models",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Device Integration",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Building the App",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Appendix - Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 161,
    "description": "Learn key principles of designing effective AI agents, and organizing a team of AI agents to perform complex, multi-step tasks. Apply these concepts to automate 6 common business processes. Learn from João Moura, founder and CEO of crewAI, and explore key components of multi-agent systems: Role-playing, Memory, Tools, Focus, Guardrails, Cooperation. Work with crewAI, an open source library designed for building multi-agent systems, to build agent crews that execute common business processes such as tailoring resumes, researching and writing articles, automating customer support, conducting outreach campaigns, planning events, and performing financial analysis.",
    "url": "https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai",
    "title": "Multi AI Agent Systems with crewAI",
    "studentProfile": "If you’ve taken some prompt engineering courses, have some familiarity with basic coding, and want to incorporate LLMs in your professional work, then this course is designed for you!",
    "learningGoals": [
      "Exceed the performance of prompting a single LLM by designing and prompting a team of AI agents through natural language.",
      "Use an open source library, crewAI, to automate repeatable, multi-step tasks like tailoring a resume to a job description; and automate business processes that are typically done by a group of people, like event planning.",
      "By creating a team of AI agents, you can define a specific role, goal, and backstory for each agent, which breaks down complex multi-step tasks and assigns them to agents that are customized to perform those tasks."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "João Moura",
        "title": "Founder and CEO of CrewAI"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Overview",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "AI Agents",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Create agents to research and write an article",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Key elements of AI agents",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Multi agent customer support automation",
        "usesCodeExample": true
      },
      {
        "duration": 3,
        "title": "Mental framework for agent creation",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Key elements of agent tools",
        "usesCodeExample": false
      },
      {
        "duration": 16,
        "title": "Tools for a customer outreach campaign",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Recap of tools",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Key elements of well defined tasks",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Automate event planning",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Recap on tasks",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Multi agent collaboration",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Mutli agent collaboration for financial analysis",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Build a crew to trailor job applications",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Next steps with AI agent systems",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Next Step: Build Advanced Multi AI Agent",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 82,
    "description": "Learn how to build multimodal search and RAG systems. RAG systems enhance an LLM by incorporating proprietary data into the prompt context. Typically, RAG applications use text documents, but, what if the desired context includes multimedia like images, audio, and video? This course covers the technical aspects of implementing RAG with multimodal data to accomplish this.\n\nLearn how multimodal models are trained through contrastive learning and implement it on a real dataset. Build any-to-any multimodal search to retrieve relevant context across different data types. Learn how LLMs are trained to understand multimodal data through visual instruction tuning and use them on multiple image reasoning examples. Implement an end-to-end multimodal RAG system that analyzes retrieved multimodal context to generate insightful answers. Explore industry applications like visually analyzing invoices and flowcharts to output structured data. Create a multi-vector recommender system that suggests relevant items by comparing their similarities across multiple modalities.\n\nAs AI systems increasingly need to process and reason over multiple data modalities, learning how to build such systems is an important skill for AI developers.\n\nThis course equips you with the key skills to embed, retrieve, and generate across different modalities. By gaining a strong foundation in multimodal AI, you’ll be prepared to build smarter search, RAG, and recommender systems.",
    "url": "https://www.deeplearning.ai/short-courses/building-multimodal-search-and-rag",
    "title": "Building Multimodal Search and RAG",
    "studentProfile": "This course is for anyone who wants to start building their own multimodal applications. Basic Python knowledge, as well as familiarity with RAG is recommended to get the most out of this course.",
    "learningGoals": [
      "Learn how multimodality works by implementing contrastive learning, and see how it can be used to build modality-independent embeddings for seamless any-to-any retrieval.",
      "Build multimodal RAG systems that retrieve multimodal context and reason over it to generate more relevant answers.",
      "Implement industry applications of multimodal search and build multi-vector recommender systems."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Sebastian Witalec",
        "title": "Head of Developer Relations at Weaviate"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 23,
        "title": "Overview of Multimodality",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Multimodal Search",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Large Multimodal Models (LMMs)",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Multimodal RAG (MM-RAG)",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Industry Applications",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Multimodal Recommender System",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 44,
    "description": "Join our new short course and learn from Jerry Liu, co-founder and CEO at LlamaIndex to start using agentic RAG, a framework designed to build research agents skilled in tool use, reasoning, and decision-making with your data. In this course: Build the simplest form of agentic RAG – a router. Given a query, the router will pick one of two query engines, Q&A or summarization, to execute a query over a single document. Add tool calling to your router agent where you will use an LLM to not only pick a function to execute but also infer an argument to pass to the function. Build a research assistant agent. Instead of tool calling in a single-shot setting, an agent is able to reason over tools in multiple steps. Build a multi-document agent where you will learn how to extend the research agent to handle multiple documents. Unlike the standard RAG pipeline—suitable for simple queries across a few documents—this intelligent approach adapts based on initial findings to enhance further data retrieval. You’ll learn to develop an autonomous research agent, enhancing your ability to engage with and analyze your data comprehensively. You’ll practice building agents capable of intelligently navigating, summarizing, and comparing information across multiple research papers from arXiv. Additionally, you’ll learn how to debug these agents, ensuring you can guide their actions effectively. Explore one of the most rapidly advancing applications of agentic AI!",
    "url": "https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex",
    "title": "Building Agentic RAG with LlamaIndex",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn how to quickly build agents that can reason over their own documents.",
    "learningGoals": [
      "Learn how to build an agent that can reason over your documents and answer complex questions.",
      "Build a router agent that can help you with Q&A and summarization tasks, and extend it to handle passing arguments to this agent.",
      "Design a research agent that handles multi-documents and learn about different ways to debug and control this agent."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Jerry Liu",
        "title": "Co-founder and CEO of LlamaIndex"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Router Query Engine",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Tool Calling",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Building an Agent Reasoning Loop",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Building a Multi-Document Agent",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 130,
    "description": "In Quantization in Depth you will build model quantization methods to shrink model weights to ¼ their original size, and apply methods to maintain the compressed model’s performance. Your ability to quantize your models can make them more accessible, and also faster at inference time. Implement and customize linear quantization from scratch so that you can study the tradeoff between space and performance, and then build a general-purpose quantizer in PyTorch that can quantize any open source model. You’ll implement techniques to compress model weights from 32 bits to 8 bits and even 2 bits. This course lets you build and customize your own linear quantizer from scratch, going beyond standard open source libraries such as PyTorch and Quanto, which are covered in the short course Quantization Fundamentals, also by Hugging Face. This course gives you the foundation to study more advanced quantization methods, some of which are recommended at the end of the course.",
    "url": "https://www.deeplearning.ai/short-courses/quantization-in-depth",
    "title": "Quantization in Depth",
    "studentProfile": "Building on the concepts introduced in Quantization Fundamentals with Hugging Face, this course will help deepen your understanding of linear quantization methods. If you’re looking to go further into quantization, this course is the perfect next step.",
    "learningGoals": [
      "Try out different variants of Linear Quantization, including symmetric vs. asymmetric mode, and different granularities like per tensor, per channel, and per group quantization.",
      "Build a general-purpose quantizer in Pytorch that can quantize the dense layers of any open source model for up to 4x compression on dense layers.",
      "Implement weights packing to pack four 2-bit weights into a single 8-bit integer."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Marc Sun",
        "title": "Machine Learning Engineer at Hugging Face"
      },
      {
        "name": "Younes Belkada",
        "title": "Machine Learning Engineer at Hugging Face"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "Overview",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "Quantize and De-quantize a Tensor",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Get the Scale and Zero Point",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Symmetric vs Asymmetric Mode",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "Finer Granularity for more Precision",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Per Channel Quantization",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Per Group Quantization",
        "usesCodeExample": true
      },
      {
        "duration": 3,
        "title": "Quantizing Weights & Activations for Inference",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Custom Build an 8-Bit Quantizer",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Replace PyTorch layers with Quantized Layers",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Quantize any Open Source PyTorch Model",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Load your Quantized Weights from HuggingFace Hub",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Weights Packing",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Packing 2-bit Weights",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Unpacking 2-Bit Weights",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Beyond Linear Quantization",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 82,
    "description": "Prompt engineering is used not only in text models but also in vision models. Depending on the vision model, they may use text prompts, but can also work with pixel coordinates, bounding boxes, or segmentation masks. In this course, you’ll learn to prompt different vision models like Meta’s Segment Anything Model (SAM), a universal image segmentation model, OWL-ViT, a zero-shot object detection model, and Stable Diffusion 2.0, a widely used diffusion model. You’ll also use a fine-tuning technique called DreamBooth to tune a diffusion model to associate a text label with an object of your preference. In detail, you’ll explore: Image Generation: Prompt with text and by adjusting hyperparameters like strength, guidance scale, and number of inference steps. Image Segmentation: Prompt with positive or negative coordinates, and with bounding box coordinates. Object detection: Prompt with natural language to produce a bounding box to isolate specific objects within images. In-painting: Combine the above techniques to replace objects within an image with generated content. Personalization with Fine-tuning: Generate custom images based on pictures of people or places that you provide, using a fine-tuning technique called DreamBooth. Iterating and Experiment Tracking: Prompting and hyperparameter tuning are iterative processes, and therefore experiment tracking can help to identify the most effective combinations. This course will use Comet, a library to track experiments and optimize visual prompt engineering workflows.",
    "url": "https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models",
    "title": "Prompt Engineering for Vision Models",
    "studentProfile": "Prompt Engineering for Vision Models is a hands-on course that helps you get started with prompting vision models. Python experience is recommended.",
    "learningGoals": [
      "Prompt vision models with text, coordinates, and bounding boxes, and tune hyper-parameters like guidance scale, strength, and number of inference steps.",
      "Replace parts of an image with generated content with in-painting, a technique that combines object detection, image segmentation, and image generation.",
      "Fine-tune a diffusion model to have even more control over your image generation to create specific images, including your own, rather than generically generated images."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Abby Morgan",
        "title": "Machine Learning Engineer at Comet"
      },
      {
        "name": "Jacques Verré",
        "title": "Head of Product at Comet"
      },
      {
        "name": "Caleb Kaiser",
        "title": "Machine Learning Engineer at Comet"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Overview",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Image Segmentation",
        "usesCodeExample": true
      },
      {
        "duration": 23,
        "title": "Object Detection",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Image Generation",
        "usesCodeExample": true
      },
      {
        "duration": 20,
        "title": "Fine-tuning",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 52,
    "description": "In this course, you’ll access Mistral AI’s collection of open source and commercial models, including the Mixtral 8x7B model, and the latest Mixtral 8x22B. You’ll learn about selecting the right model for your use case, and get hands-on with features like effective prompting techniques, function calling, JSON mode, and Retrieval Augmented Generation (RAG). By the end of this course, you’ll be equipped to leverage Mistral AI’s leading open source and commercial models.",
    "url": "https://www.deeplearning.ai/short-courses/getting-started-with-mistral",
    "title": "Getting Started with Mistral",
    "studentProfile": "_Getting Started with Mistral_ is a beginner-friendly course and it’s suitable for anyone who wants to learn about and use Mistral AI’s collection of advanced open source and commercial LLMs. If you have taken [_ChatGPT Prompt Engineering for Developers_] or [_Prompt Engineering with Llama 2_], this is a great next step!",
    "learningGoals": [
      "Explore Mistral’s three open source models (Mistral 7B, Mixtral 8x7B, and the latest Mixtral 8x22B), and three commercial models (small, medium, and large), which Mistral provides access to via web interface and API calls.",
      "Leverage Mistral’s JSON mode to generate LLM responses in a structured JSON format, enabling integration of LLM outputs into larger software applications.",
      "Use Mistral’s API to call user-defined Python functions for tasks like web searches or retrieving text from databases, enhancing the LLM’s ability to find relevant information to answer user queries."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Sophia Yang",
        "title": "Head of Developer Relations at Mistral AI"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Overview",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Prompting",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Model Selection",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Function Calling",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "RAG from Scratch",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Chatbot",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 64,
    "description": "Generative AI models, like large language models, often exceed the capabilities of consumer-grade hardware and are expensive to run. Compressing models through methods such as quantization makes them more efficient, faster, and accessible. This allows them to run on a wide variety of devices, including smartphones, personal computers, and edge devices, and minimizes performance degradation.\n\nJoin this course to:\n\n* Quantize any open source model with linear quantization using the Quanto library.\n* Get an overview of how linear quantization is implemented. This form of quantization can be applied to compress any model, including LLMs, vision models, etc.\n* Apply “downcasting,” another form of quantization, with the Transformers library, which enables you to load models in about half their normal size in the BFloat16 data type.\n\nBy the end of this course, you will have a foundation in quantization techniques and be able to apply them to compress and optimize your own generative AI models, making them more accessible and efficient.",
    "url": "https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face",
    "title": "Quantization Fundamentals with Hugging Face",
    "studentProfile": "This is an introduction to the fundamental concepts of quantization for learners with a basic understanding of machine learning concepts and some experience with PyTorch, who is interested in learning about model quantization in generative AI.",
    "learningGoals": [
      "Learn how to compress models with the Hugging Face Transformers library and the Quanto library.",
      "Learn about linear quantization, a simple yet effective method for compressing models.",
      "Practice quantizing open source multimodal and language models."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Younes Belkada",
        "title": "Machine Learning Engineer at Hugging Face"
      },
      {
        "name": "Marc Sun",
        "title": "Machine Learning Engineer at Hugging Face"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Handling Big Models",
        "usesCodeExample": false
      },
      {
        "duration": 17,
        "title": "Data Types and Sizes",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Loading Models by data type",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Quantization Theory",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Quantization of LLMs",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 72,
    "description": "Enhancing a RAG system’s performance depends on efficiently processing diverse unstructured data sources. In this course, you’ll learn techniques for representing all sorts of unstructured data, like text, images, and tables, from many different sources and implement them to extend your LLM RAG pipeline to include Excel, Word, PowerPoint, PDF, and EPUB files.\n\nJoin this course and learn:\n- How to preprocess data for your LLM application development, focusing on how to work with different document types.\n- How to extract and normalize various documents into a common JSON format and enrich it with metadata to improve search results.\n- Techniques for document image analysis, including layout detection and vision transformers, to extract and understand PDFs, images, and tables.\n- How to build a RAG bot that is able to ingest different documents like PDFs, PowerPoints, and Markdown files.\n\nApply the skills you’ll learn in this course to real-world scenarios, enhancing your RAG application and expanding its versatility.",
    "url": "https://www.deeplearning.ai/short-courses/preprocessing-unstructured-data-for-llm-applications",
    "title": "Preprocessing Unstructured Data for LLM Applications",
    "studentProfile": "Anyone who is interested in learning how to effectively process and use diverse data types and formats to build high-performing LLM RAG systems.",
    "learningGoals": [
      "Learn to extract and normalize content from a wide variety of document types, such as PDFs, PowerPoints, Word, and HTML files, tables, and images to expand the information accessible to your LLM.",
      "Enrich your content with metadata, enhancing retrieval augmented generation (RAG) results and supporting more nuanced search capabilities.",
      "Explore document image analysis techniques like layout detection and vision and table transformers, and learn how to apply these methods to preprocess PDFs, images, and tables."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Matt Robinson",
        "title": "Head of Product at Unstructured"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 3,
        "title": "Overview of LLM Data Preprocessing",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Normalizing the Content",
        "usesCodeExample": true
      },
      {
        "duration": 21,
        "title": "Metadata Extraction and Chunking",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Preprocessing PDFs and Images",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Extracting Tables",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Build Your Own RAG Bot",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Appendix - Tips and Help",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 79,
    "description": "Learn how to test and find vulnerabilities in your LLM applications to make them safer. In this course, you’ll attack various chatbot applications using prompt injections to see how the system reacts and understand security failures. LLM failures can lead to legal liability, reputational damage, and costly service disruptions. This course helps you mitigate these risks proactively. Learn industry-proven red teaming techniques to proactively test, attack, and improve the robustness of your LLM applications.\n\nIn this course:\n- Explore the nuances of LLM performance evaluation, and understand the differences between benchmarking foundation models and testing LLM applications.\n- Get an overview of fundamental LLM application vulnerabilities and how they affect real-world deployments.\n- Gain hands-on experience with both manual and automated LLM red-teaming methods.\n- See a full demonstration of red-teaming assessment, and apply the concepts and techniques covered throughout the course.\n\nAfter completing this course, you will have a fundamental understanding of how to experiment with LLM vulnerability identification and evaluation on your own applications.",
    "url": "https://www.deeplearning.ai/short-courses/red-teaming-llm-applications",
    "title": "Red Teaming LLM Applications",
    "studentProfile": "Red Teaming LLM Applications is a beginner-friendly course. Basic Python knowledge is recommended to get the most out of this course.",
    "learningGoals": [
      "Learn to identify and evaluate vulnerabilities in large language model (LLM) applications.",
      "Apply red teaming techniques from cybersecurity to ensure the safety and reliability of your LLM application.",
      "Use an open source library from Giskard to help automate LLM red-teaming methods."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Matteo Dora",
        "title": "Lead LLM Safety Researcher at Giskard"
      },
      {
        "name": "Luca Martial",
        "title": "Product Lead at Giskard"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 18,
        "title": "Overview of LLM Vulnerabilities",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Red Teaming LLMs",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Red Teaming at Scale",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Red Teaming LLMs with LLMs",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "A Full Red Teaming Assessment",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 38,
    "description": "Get started building full-stack RAG web applications. This course introduces the basics of Retrieval Augment Generation (RAG), including its practical implementation with JavaScript. You’ll assemble an intelligent agent capable of running its own queries. This course will guide you through the process of building a full-stack JavaScript web application, from the API server to a React component that queries your data. You’ll also learn to develop persistent chat that streams your answers to an interactive front-end in real time.\n\nLearn from Laurie Voss, VP of developer relations at LlamaIndex, web developer and the co-founder of npm, the central registry of JavaScript packages.\n\nIn this course:\n- Learn to build a RAG application in JavaScript for querying your own data.\n- Develop tools to interact with multiple data sources using a router query engine that intelligently selects the right tool for your queries.\n- Build a full-stack web application step by step, starting with a backend web app and progressing to an interactive React frontend that calls your API to display query results.\n- Learn about production-ready techniques, including persisting your data, chatting with your data, and streaming responses for multiple queries.\n- Create a user-friendly web app that can chat with data using the create-llama command line tool from LlamaIndex.\n\nStart building RAG web applications in JavaScript that allow you to interact with your data using LlamaIndex.",
    "url": "https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex",
    "title": "JavaScript RAG Web Apps with LlamaIndex",
    "studentProfile": "Anyone who has basic JavaScript knowledge who wants to explore RAG application development and to quickly build a full-stack web application using LlamaIndex!",
    "learningGoals": [
      "Learn how to build a RAG application in JavaScript, and use an intelligent agent that discerns and selects from multiple data sources to answer your queries.",
      "Build a full-stack web app with an interactive frontend component that interacts and chats with your data.",
      "Learn how to persist your data, enable chatting with your data and make streaming responses possible, all implemented using the create-llama command-line tool."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Laurie Voss",
        "title": "VP of Developer Relations at LlamaIndex"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 8,
        "title": "Getting started with RAG",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Build a full-stack web app",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Advanced queries with Agents",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Production-ready techniques",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 150,
    "description": "Join our new short course, Efficiently Serving Large Language Models, to build a ground-up understanding of how to serve LLM applications from Travis Addair, CTO at Predibase. Whether you’re ready to launch your own application or just getting started building it, the topics you’ll explore in this course will deepen your foundational knowledge of how LLMs work, and help you better understand the performance trade-offs you must consider when building LLM applications that will serve large numbers of users. You’ll walk through the most important optimizations that allow LLM vendors to efficiently serve models to many customers, including strategies for working with multiple fine-tuned models at once. In this course, you will: learn how auto-regressive large language models generate text one token at a time; implement the foundational elements of a modern LLM inference stack in code, including KV caching, continuous batching, and model quantization, and benchmark their impacts on inference throughput and latency; explore the details of how LoRA adapters work, and learn how batching techniques allow different LoRA adapters to be served to multiple customers simultaneously; get hands-on with Predibase’s LoRAX framework inference server to see these optimization techniques implemented in a real world LLM inference server. Knowing more about how LLM servers operate under the hood will greatly enhance your understanding of the options you have to increase the performance and efficiency of your LLM-powered applications.",
    "url": "https://www.deeplearning.ai/short-courses/efficiently-serving-llms",
    "title": "Efficiently Serving LLMs",
    "studentProfile": "Anyone who wants to understand the components, techniques, and tradeoffs of efficiently serving LLM applications, and gain a step-by-step understanding of how they work. This course relies on intermediate Python knowledge and demonstrates real-world techniques and applications.",
    "learningGoals": [
      "Learn how Large Language Models (LLMs) repeatedly predict the next token, and how techniques like KV caching can greatly speed up text generation.",
      "Write code to efficiently serve LLM applications to a large number of users, and examine the tradeoffs between quickly returning the output of the model and serving many users at once.",
      "Explore the fundamentals of Low Rank Adapters (LoRA) and see how Predibase builds their LoRAX framework inference server to serve multiple fine-tuned models at once."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Travis Addair",
        "title": "Co-Founder and CTO at Predibase"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 20,
        "title": "Text Generation",
        "usesCodeExample": true
      },
      {
        "duration": 22,
        "title": "Batching",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Continuous Batching",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "Quantization",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Low-Rank Adaptation",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "Multi-LoRA inference",
        "usesCodeExample": true
      },
      {
        "duration": 30,
        "title": "LoRAX",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 114,
    "description": "Knowledge graphs are used in development to structure complex data relationships, drive intelligent search functionality, and build powerful AI applications that can reason over different data types. Knowledge graphs can connect data from both structured and unstructured sources (databases, documents, etc.), providing an intuitive and flexible way to model complex, real-world scenarios.\n\nUnlike tables or simple lists, knowledge graphs can capture the meaning and context behind the data, allowing you to uncover insights and connections that would be difficult to find with conventional databases. This rich, structured context is ideal for improving the output of large language models (LLMs), because you can build more relevant context for the model than with semantic search alone.\n\nThis course will teach you how to leverage knowledge graphs within retrieval augmented generation (RAG) applications. You’ll learn to:\n\n- Understand the basics of how knowledge graphs store data by using nodes to represent entities and edges to represent relationships between nodes.\n- Use Neo4j’s query language, Cypher, to retrieve information from a fun graph of movie and actor data.\n- Add a vector index to a knowledge graph to represent unstructured text data and find relevant texts using vector similarity search.\n- Build a knowledge graph of text documents from scratch, using publicly available financial and investment documents as the demo use case\n- Explore advanced techniques for connecting multiple knowledge graphs and using complex queries for comprehensive data retrieval.\n- Write advanced Cypher queries to retrieve relevant information from the graph and format it for inclusion in your prompt to an LLM.\n\nAfter course completion, you’ll be well-equipped to use knowledge graphs to uncover deeper insights in your data, and enhance the performance of LLMs with structured, relevant context.",
    "url": "https://www.deeplearning.ai/short-courses/knowledge-graphs-rag",
    "title": "Knowledge Graphs for RAG",
    "studentProfile": "Anyone who wants to understand how knowledge graphs work, how to build with them, and create better RAG applications. We recommend familiarity with LangChain or taking LangChain: Chat with Your Data prior to this course.",
    "learningGoals": [
      "Use Neo4j’s query language Cypher to manage and retrieve data stored in knowledge graphs.",
      "Write knowledge graph queries that find and format text data to provide more relevant context to LLMs for Retrieval Augmented Generation.",
      "Build a question-answering system using Neo4j and LangChain to chat with a knowledge graph of structured text documents."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Andreas Kollegger",
        "title": "Developer Relations for Generative AI at Neo4j"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Knowledge Graph Fundamentals",
        "usesCodeExample": false
      },
      {
        "duration": 19,
        "title": "Querying Knowledge Graphs",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Preparing Text for RAG",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Constructing a Knowledge Graph from Text Documents",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Adding Relationships to the SEC Knowledge Graph",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Expanding the SEC Knowledge Graph",
        "usesCodeExample": true
      },
      {
        "duration": 23,
        "title": "Chatting with the Knowledge Graph",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 123,
    "description": "The availability of models and their weights for anyone to download enables a broader range of developers to innovate and create. In this course, you’ll select open source models from Hugging Face Hub to perform NLP, audio, image and multimodal tasks using the Hugging Face transformers library. Easily package your code into a user-friendly app that you can run on the cloud using Gradio and Hugging Face Spaces. You will use the transformers library to turn a small language model into a chatbot capable of multi-turn conversations to answer follow-up questions, translate between languages, summarize documents, and measure the similarity between two pieces of text, convert audio to text with Automatic Speech Recognition (ASR), perform zero-shot audio classification, generate audio narration describing images, identify objects in images with zero-shot image segmentation, implement visual question answering and more. Share your AI app using Gradio and Hugging Face Spaces to run your applications with a friendly interface on the cloud or as an API. The course provides the building blocks to build AI-enabled applications.",
    "url": "https://www.deeplearning.ai/short-courses/open-source-models-hugging-face",
    "title": "Open Source Models with Hugging Face",
    "studentProfile": "Anyone who wants to get started building AI applications quickly and easily using open source models.",
    "learningGoals": [
      "Find and filter open source models on Hugging Face Hub based on task, rankings, and memory requirements.",
      "Write just a few lines of code using the transformers library to perform text, audio, image, and multimodal tasks.",
      "Easily share your AI apps with a user-friendly interface or via API and run them on the cloud using Gradio and Hugging Face Spaces."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Maria Khalusova",
        "title": "Member of Technical Staff at Hugging Face"
      },
      {
        "name": "Marc Sun",
        "title": "Machine Learning Engineer at Hugging Face"
      },
      {
        "name": "Younes Belkada",
        "title": "Machine Learning Engineer at Hugging Face"
      }
    ],
    "courseItems": [
      {
        "duration": 5,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 5,
        "title": "Selecting models",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Natural Language Processing (NLP)",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Translation and Summarization",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Sentence Embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Zero-Shot Audio Classification",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Automatic Speech Recognition",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "Text to Speech",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Object Detection",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Image Segmentation",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Image Retrieval",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Image Captioning",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Multimodal Visual Question Answering",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Zero-Shot Image Classification",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Deployment",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 113,
    "description": "Open up your prompt engineering to the Llama 2 & 3 collection of models! Learn best practices for prompting and building applications with these powerful open commercial license models.\n\nInteract with the Llama 2 and Llama 3 models with a simple API call, and explore the differences in output between models for a variety of tasks.\n\nWhat you’ll do:\n* Learn best practices for prompting and selecting among the Llama 2 & 3 models by using them as a personal assistant to help you complete day-to-day tasks.\n* Experiment with advanced prompt engineering techniques, like few-shot prompting to get Llama 2 to classify the sentiment of text messages, and chain-of-thought prompting to solve logic problems.\n* Treat Code Llama as a pair programming partner to both learn to write and improve code.\n* Promote safe and responsible use of LLMs by having Llama Guard check user prompts and model responses for harmful content.",
    "url": "https://www.deeplearning.ai/short-courses/prompt-engineering-with-llama-2",
    "title": "Prompt Engineering with Llama 2 & 3",
    "studentProfile": "Anyone who is interested in learning prompt engineering and wants to try out Meta Llama 2 and Llama 3 models.",
    "learningGoals": [
      "Learn best practices specific to prompting Llama 2 & 3 models.",
      "Interact with Meta Llama 2 Chat, Code Llama, and Llama Guard models.",
      "See how you can build safe, responsible AI applications using the Llama Guard model."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Amit Sangani",
        "title": "Senior Director of Partner Engineering of Meta"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Overview of Llama Models",
        "usesCodeExample": false
      },
      {
        "duration": 16,
        "title": "Getting Started with Llama 2 & 3",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Multi-turn Conversations",
        "usesCodeExample": true
      },
      {
        "duration": 22,
        "title": "Prompt Engineering Techniques",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Comparing Different Llama 2 & 3 models",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Code Llama",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Llama Guard",
        "usesCodeExample": true
      },
      {
        "duration": 3,
        "title": "Walkthrough of Llama Helper Function (Optional)",
        "usesCodeExample": true
      },
      {
        "duration": 2,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 129,
    "description": "In this course, you’ll learn how to deploy a large language model-based application into production using serverless technology. A serverless architecture enables you to quickly deploy your applications without the need to manage and scale the infrastructure that it runs on. You’ll learn to summarize audio files by pairing an LLM with an automatic speech recognition (ASR) model. Through hands-on exercises, you’ll build an event-driven system that automatically detects incoming customer inquiries, transcribes them with ASR and summarizes them with an LLM, using Amazon Bedrock. After course completion, you’ll know how to: Prompt an LLM and customize its responses using Amazon Bedrock. Convert audio recordings into written transcripts with Amazon Transcribe, and then summarize these transcripts using an LLM, Amazon Titan. Enable logging for all the calls you make to LLMs to help you maintain security, audit, and compliance standards. Deploy this audio summarizer as an event-driven serverless workflow using AWS Lambda. You’ll work with the Amazon Titan model, but in practice Amazon Bedrock allows you to use any model you prefer. Start building serverless LLM applications with Amazon Bedrock and deploy your apps in just days.",
    "url": "https://www.deeplearning.ai/short-courses/serverless-llm-apps-amazon-bedrock",
    "title": "Serverless LLM Apps with Amazon Bedrock",
    "studentProfile": "Anyone who is familiar with Python, AWS services, and wants to learn to quickly deploy LLM apps with Amazon Bedrock.",
    "learningGoals": [
      "Learn how to prompt and customize your LLM responses using Amazon Bedrock.",
      "Summarize audio conversations by first transcribing an audio file and passing the transcription to an LLM.",
      "Deploy an event-driven audio summarizer that runs as new audio files are uploaded using a serverless architecture."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Mike Chambers",
        "title": "Developer Advocate for Generative AI at AWS, Co-instructor of Generative AI with Large Language Models"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 21,
        "title": "Your first generations with Amazon Bedrock",
        "usesCodeExample": true
      },
      {
        "duration": 30,
        "title": "Summarize an audio file",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Enable logging",
        "usesCodeExample": true
      },
      {
        "duration": 43,
        "title": "Deploy an AWS Lambda function",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Event-driven generation",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 73,
    "description": "Vector databases use embeddings to capture the meaning of data, gauge the similarity between different pairs of vectors, and navigate large datasets to identify the most similar vectors. In the context of large language models, the primary use of vector databases is retrieval augmented generation (RAG), where text embeddings are stored and retrieved for specific queries. However, the versatility of vector databases extends beyond RAG and makes it possible to build a wide range of applications quickly with minimal coding. In this course, you’ll explore the implementation of six applications using vector databases: Semantic Search, RAG, Recommender System, Hybrid Search, Facial Similarity, and Anomaly Detection. After taking this course, you’ll be equipped with new ideas for building applications with any vector database.",
    "url": "https://www.deeplearning.ai/short-courses/building-applications-vector-databases",
    "title": "Building Applications with Vector Databases",
    "studentProfile": "Anyone with beginner Python, basic machine learning and large language models knowledge who wants to learn applications of vector databases.",
    "learningGoals": [
      "Learn to create six exciting applications of vector databases and implement them using Pinecone.",
      "Build a hybrid search app that combines both text and images for improved multimodal search results.",
      "Learn how to build an app that measures and ranks facial similarity."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Tim Tully",
        "title": "Board member at Pinecone"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Semantic Search",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Retrieval Augmented Generation (RAG)",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Recommender Systems",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Hybrid Search",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Facial Similarity Search",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Anomaly Detection",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 52,
    "description": "In this course, you will learn how to create a continuous integration (CI) workflow to evaluate your LLM applications at every change for faster, safer, and more efficient application development. When building applications with generative AI, model behavior is less predictable than traditional software. That’s why systematic testing can make an even bigger difference in saving you development time and cost. Continuous integration, a key part of LLMOps, is the practice of making small changes to software in development and thoroughly testing them to catch issues early when they are easier to fix. With a robust automated testing pipeline, you’ll be able to isolate bugs before they accumulate – when they’re easier and less costly to fix. Automated testing lets your team focus on building new features, so that you can iterate and ship products faster.",
    "url": "https://www.deeplearning.ai/short-courses/automated-testing-llmops",
    "title": "Automated Testing for LLMOps",
    "studentProfile": "Anyone with basic Python knowledge and familiarity with building LLM-based applications.",
    "learningGoals": [
      "Learn how LLM-based testing differs from traditional software testing and implement rules-based testing to assess your LLM application.",
      "Build model-graded evaluations to test your LLM application using an evaluation LLM.",
      "Automate your evals (rules-based and model-graded) using continuous integration tools from CircleCI."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Rob Zuber",
        "title": "CTO at CircleCI"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 4,
        "title": "Introduction to Continuous Integration (CI)",
        "usesCodeExample": false
      },
      {
        "duration": 23,
        "title": "Overview of Automated Evals",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Automating Model-Graded Evals",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Comprehensive Testing Framework",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Optional: Exploring the CircleCI config file",
        "usesCodeExample": true
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 81,
    "description": "In this course, you’ll go through the LLMOps pipeline of pre-processing training data for supervised instruction tuning, and adapt a supervised tuning pipeline to train and deploy a custom LLM. This is useful in creating an LLM workflow for your specific application. For example, creating a question-answer chatbot tailored to answer Python coding questions, which you’ll do in this course.\n\nThrough the course, you’ll go through key steps of creating the LLMOps pipeline:\n\n- Retrieve and transform training data for supervised fine-tuning of an LLM.\n- Version your data and tuned models to track your tuning experiments.\n- Configure an open-source supervised tuning pipeline and then execute that pipeline to train and then deploy a tuned LLM.\n- Output and study safety scores to responsibly monitor and filter your LLM application’s behavior.\n- Try out the tuned and deployed LLM yourself in the classroom!\n- Tools you’ll practice with include BigQuery data warehouse, the open-source Kubeflow Pipelines, and Google Cloud.",
    "url": "https://www.deeplearning.ai/short-courses/llmops",
    "title": "LLMOps",
    "studentProfile": "Anyone who wants to learn to tune an LLM, and learn to work with and build an LLMOps pipeline.",
    "learningGoals": [
      "Adapt an open source pipeline that applies supervised fine-tuning on an LLM to better answer user questions.",
      "Learn best practices, including versioning your data and your models, and pre-process large datasets inside a data warehouse.",
      "Learn responsible AI by outputting safety scores on sub-categories of harmful content."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Erwin Huizenga",
        "title": "Developer Advocate for Generative AI on Google Cloud"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "The Fundamentals",
        "usesCodeExample": false
      },
      {
        "duration": 21,
        "title": "Data Preparation",
        "usesCodeExample": true
      },
      {
        "duration": 21,
        "title": "Automation and Orchestration with Pipelines",
        "usesCodeExample": true
      },
      {
        "duration": 21,
        "title": "Prediction, Prompts, Safety",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 64,
    "description": "JavaScript is the world’s most popular programming language, and now developers can program in JavaScript to build powerful LLM apps. This course will show webdevs how to expand their toolkits with LangChain.js, a popular JavaScript framework for building with LLMs, and will cover useful concepts for creating powerful, context-aware applications. By taking this course, you will: Learn to use LangChain’s underlying abstractions to build your own JavaScript apps, Understand the basics of retrieval augmented generation (RAG), Have the structure of a basic conversational retrieval system that you can use for building your own chatbots.",
    "url": "https://www.deeplearning.ai/short-courses/build-llm-apps-with-langchain-js",
    "title": "Build LLM Apps with LangChain.js",
    "studentProfile": "Anyone with intermediate JavaScript knowledge and wants to build machine learning applications.",
    "learningGoals": [
      "Understand the fundamentals of using LangChain’s JavaScript library to orchestrate and chain different modules together.",
      "Learn the basics of loading and preparing data to provide as context to effectively customize LLM generations.",
      "Learn techniques to retrieve and present data to the LLM in useful ways for a conversational retrieval chain."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Jacob Lee",
        "title": "LangChain.js Lead Maintainer and Founding Software Engineer"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "Building Blocks",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Loading and preparing data",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Vectorstores and embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Question answering",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Conversational question answering",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Shipping as a web API",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 52,
    "description": "Information Retrieval (IR) and Retrieval Augmented Generation (RAG) are only effective if the information retrieved from a database as a result of a query is relevant to the query and its application. Too often, queries return semantically similar results but don’t answer the question posed. They may also return irrelevant material which can distract the LLM from the correct results. This course teaches advanced retrieval techniques to improve the relevancy of retrieved results. The techniques covered include Query Expansion, Cross-encoder reranking, and Training and utilizing Embedding Adapters.",
    "url": "https://www.deeplearning.ai/short-courses/advanced-retrieval-for-ai",
    "title": "Advanced Retrieval for AI with Chroma",
    "studentProfile": "Anyone who has intermediate Python knowledge and wants to learn advanced retrieval techniques for retrieving data from their vector database.",
    "learningGoals": [
      "Learn to recognize when queries are producing poor results.",
      "Learn to use a large language model (LLM) to improve your queries.",
      "Learn to fine-tune your embeddings with user feedback."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Anton Troynikov",
        "title": "Co-founder, Chroma"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Overview of embeddings-based retrieval",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Pitfalls of retrieval - when simple vector search fails",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Query Expansion",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Cross-encoder re-ranking",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Embedding adaptors",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Other Techniques",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 72,
    "description": "Large language models (LLMs) are trained on human-generated text, but additional methods are needed to align an LLM with human values and preferences. Reinforcement Learning from Human Feedback (RLHF) is currently the main method for aligning LLMs with human values and preferences. RLHF is also used for further tuning a base LLM to align with values and preferences that are specific to your use case. In this course, you will gain a conceptual understanding of the RLHF training process, and then practice applying RLHF to tune an LLM. You will explore the two datasets that are used in RLHF training: the “preference” and “prompt” datasets. You will use the open source Google Cloud Pipeline Components Library, to fine-tune the Llama 2 model with RLHF. You will assess the tuned LLM against the original base model by comparing loss curves and using the “Side-by-Side (SxS)” method.",
    "url": "https://www.deeplearning.ai/short-courses/reinforcement-learning-from-human-feedback",
    "title": "Reinforcement Learning from Human Feedback",
    "studentProfile": "Anyone with intermediate Python knowledge who’s interested in learning about using the Reinforcement Learning from Human Feedback technique.",
    "learningGoals": [
      "Get a conceptual understanding of Reinforcement Learning from Human Feedback (RLHF), as well as the datasets needed for this technique",
      "Fine-tune the Llama 2 model using RLHF with the open source Google Cloud Pipeline Components Library",
      "Evaluate tuned model performance against the base model with evaluation methods"
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Nikita Namjoshi",
        "title": "Developer Advocate at Google Cloud"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "How does RLHF work",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Datasets for RL training",
        "usesCodeExample": true
      },
      {
        "duration": 24,
        "title": "Tune an LLM with RLHF",
        "usesCodeExample": true
      },
      {
        "duration": 22,
        "title": "Evaluate the tuned model",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Google Cloud Setup",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 115,
    "description": "Retrieval Augmented Generation (RAG) stands out as one of the most popular use cases of large language models (LLMs). This method facilitates the integration of an LLM with an organization’s proprietary data. To successfully implement RAG, it is essential to enhance retrieval techniques for obtaining coherent contexts and employ effective evaluation metrics. In this course, we’ll explore: Two advanced retrieval methods: Sentence-window retrieval and auto-merging retrieval that perform better compared to the baseline RAG pipeline. Evaluation and experiment tracking: A way evaluate and iteratively improve your RAG pipeline’s performance. The RAG triad: Context Relevance, Groundedness, and Answer Relevance, which are methods to evaluate the relevance and truthfulness of your LLM’s response.",
    "url": "https://www.deeplearning.ai/short-courses/building-evaluating-advanced-rag",
    "title": "Building and Evaluating Advanced RAG Applications",
    "studentProfile": "Anyone with basic Python knowledge interested in how to effectively employ the latest methods in Retrieval Augmented Generation (RAG).",
    "learningGoals": [
      "Learn methods like sentence-window retrieval and auto-merging retrieval, improving your RAG pipeline’s performance beyond the baseline.",
      "Learn evaluation best practices to streamline your process, and iteratively build a robust system.",
      "Dive into the RAG triad for evaluating the relevance and truthfulness of an LLM’s response: Context Relevance, Groundedness, and Answer Relevance."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Jerry Liu",
        "title": "Co-founder and CEO of LlamaIndex"
      },
      {
        "name": "Anupam Datta",
        "title": "Co-founder and chief scientist of TruEra"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Advanced RAG Pipeline",
        "usesCodeExample": true
      },
      {
        "duration": 42,
        "title": "RAG Triad of metrics",
        "usesCodeExample": true
      },
      {
        "duration": 29,
        "title": "Sentence-window retrieval",
        "usesCodeExample": true
      },
      {
        "duration": 21,
        "title": "Auto-merging retrieval",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 119,
    "description": "It’s always crucial to address and monitor safety and quality concerns in your applications. Building LLM applications poses special challenges.\n\nIn this course, you’ll explore new metrics and best practices to monitor your LLM systems and ensure safety and quality. You’ll learn how to:\n\n- Identify hallucinations with methods like SelfCheckGPT\n- Detect jailbreaks (prompts that attempt to manipulate LLM responses) using sentiment analysis and implicit toxicity detection models.\n- Identify data leakage using entity recognition and vector similarity analysis.\n- Build your own monitoring system to evaluate app safety and security over time.\n\nUpon completing the course, you’ll have the ability to identify common security concerns in LLM-based applications, and be able to customize your safety and security evaluation tools to the LLM that you’re using for your application.",
    "url": "https://www.deeplearning.ai/short-courses/quality-safety-llm-applications",
    "title": "Quality and Safety for LLM Applications",
    "studentProfile": "Anyone with basic Python knowledge interested in mitigating issues like hallucinations, prompt injections, and toxic outputs.",
    "learningGoals": [
      "Monitor and enhance security measures over time to safeguard your LLM applications.",
      "Detect and prevent critical security threats like hallucinations, jailbreaks, and data leakage.",
      "Explore real-world scenarios to prepare for potential risks and vulnerabilities."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Bernease Herman",
        "title": "Data Scientist at WhyLabs"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Overview",
        "usesCodeExample": true
      },
      {
        "duration": 25,
        "title": "Hallucinations",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "Data Leakage",
        "usesCodeExample": true
      },
      {
        "duration": 26,
        "title": "Refusals and prompt injections",
        "usesCodeExample": true
      },
      {
        "duration": 28,
        "title": "Passive and active monitoring",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 55,
    "description": "Vector databases play a pivotal role across various fields, such as natural language processing, image recognition, recommender systems and semantic search, and have gained more importance with the growing adoption of LLMs.\n\nThese databases are exceptionally valuable as they provide LLMs with access to real-time proprietary data, enabling the development of Retrieval Augmented Generation (RAG) applications.\n\nAt their core, vector databases rely on the use of embeddings to capture the meaning of data and gauge the similarity between different pairs of vectors and sift through extensive datasets, identifying the most similar vectors.\n\nThis course will help you gain the knowledge to make informed decisions about when to apply vector databases to your applications. You’ll explore:\n\n- How to use vector databases and LLMs to gain deeper insights into your data.\n- Build labs that show how to form embeddings and use several search techniques to find similar embeddings.\n- Explore algorithms for fast searches through vast datasets and build applications ranging from RAG to multilingual search.",
    "url": "https://www.deeplearning.ai/short-courses/vector-databases-embeddings-applications",
    "title": "Vector Databases: from Embeddings to Applications",
    "studentProfile": "Anyone who’s interested in understanding and applying vector databases in their applications.",
    "learningGoals": [
      "Build efficient, practical applications including hybrid and multilingual searches, for diverse industries.",
      "Understand vector databases and use them to develop GenAI applications without needing to train or fine-tune an LLM yourself.",
      "Learn to discern when best to apply a vector database to your application."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Sebastian Witalec",
        "title": "Head of Developer Relations at Weaviate"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 11,
        "title": "How to Obtain Vector Representations of Data",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Search for Similar Vectors",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "Approximate nearest neighbours",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Vector Databases",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Sparse, Dense, and Hybrid Search",
        "usesCodeExample": true
      },
      {
        "duration": 6,
        "title": "Application - Multilingual Search",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 104,
    "description": "The landscape of LLMs and the libraries that support them has evolved rapidly in recent months. This course is designed to keep you ahead of these changes. You’ll explore new advancements like ChatGPT’s function calling capability, and build a conversational agent using a new syntax called LangChain Expression Language (LCEL) for tasks like tagging, extraction, tool selection, and routing. After taking this course, you’ll know how to: Generate structured output, including function calls, using LLMs; Use LCEL, which simplifies the customization of chains and agents, to build applications; Apply function calling to tasks like tagging and data extraction; Understand tool selection and routing using LangChain tools and LLM function calling – and much more. Start applying these new capabilities to build and improve your applications today.",
    "url": "https://www.deeplearning.ai/short-courses/functions-tools-agents-langchain",
    "title": "Functions, Tools and Agents with LangChain",
    "studentProfile": "Anyone who’s interested in learning about the latest tools to build LLM-based applications. Basic Python knowledge and a familiarity with writing prompts for LLMs is recommended.",
    "learningGoals": [
      "Learn about the most recent advancements in LLM APIs.",
      "Use LangChain Expression Language (LCEL), a new syntax to compose and customize chains and agents faster.",
      "Apply these new capabilities by building up a conversational agent."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Harrison Chase",
        "title": "Co-Founder and CEO of LangChain"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 13,
        "title": "OpenAI Function Calling",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "LangChain Expression Language (LCEL)",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "OpenAI Function Calling in LangChain",
        "usesCodeExample": true
      },
      {
        "duration": 24,
        "title": "Tagging and Extraction",
        "usesCodeExample": true
      },
      {
        "duration": 17,
        "title": "Tools and Routing",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Conversational Agent",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 64,
    "description": "AI pair programming is being rapidly adopted by developers to help with tasks across the tech stack, from catching bugs to quickly inserting entire code snippets. Learn how LLMs can enhance, debug, and document your code in this new course built in collaboration with Google. Get free access to Google’s PaLM API and get hands-on experience that you can apply to your own projects. You’ll learn how to use an LLM in pair programming to: Simplify and improve your code, Write test cases, Debug and refactor your code, Explain and document any complex code written in any coding language.",
    "url": "https://www.deeplearning.ai/short-courses/pair-programming-llm",
    "title": "Pair Programming with a Large Language Model",
    "studentProfile": "Anyone with basic Python knowledge interested in using LLMs for pair programming.",
    "learningGoals": [
      "Use LLMs to simplify your code and become a more productive software engineer",
      "Reduce technical debt by explaining and documenting a complex existing code base",
      "Get free access to the PaLM API for use throughout the course"
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Laurence Moroney",
        "title": "Former AI Lead at Google"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 15,
        "title": "Getting Started",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Using a String Template",
        "usesCodeExample": true
      },
      {
        "duration": 27,
        "title": "Pair Programing Scenarios",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Technical Debt",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 84,
    "description": "The Vertex AI Text-Embeddings API enhances the process of generating text embeddings. These text embeddings, which are numerical representations of text, play a pivotal role in many tasks involving the identification of similar items, like Google searches, online shopping recommendations, and personalized music suggestions.\n\nDuring this course, you’ll use text embeddings for tasks like classification, outlier detection, text clustering and semantic search. You’ll combine semantic search with the text generation capabilities of an LLM to build a question-answering systems using Google Cloud’s Vertex AI.\n\nYou’ll also explore:\n- The properties of word and sentence embeddings\n- How embeddings can be used to measure the semantic similarity between two pieces of text\n- How to apply text embeddings for tasks such as classification, clustering, and outlier detection\n- Modify the text generation behavior of an LLM by adjusting the parameters temperature, top-k, and top-p\n- How to apply the open source ScaNN (Scalable Nearest Neighbors) library for efficient semantic search\n- How to build a Q&A system by combining semantic search with an LLM\n\nUpon successful completion of this course, you will grasp the underlying concepts of using text embeddings, and will also gain proficiency in generating embeddings and integrating them into common LLM applications.",
    "url": "https://www.deeplearning.ai/short-courses/google-cloud-vertex-ai",
    "title": "Understanding and Applying Text Embeddings",
    "studentProfile": "Anyone with basic Python knowledge who wants to learn about text embeddings and how to apply them to common NLP tasks.",
    "learningGoals": [
      "Use text embeddings to capture the meaning of sentences and paragraphs",
      "Apply text embeddings for tasks like text clustering, classification, and outlier detection",
      "Use Google Cloud’s Vertex AI to build a question answering system"
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Nikita Namjoshi",
        "title": "Developer Advocate at Google Cloud"
      },
      {
        "name": "Andrew Ng",
        "title": "Founder, DeepLearning.AI; Co-founder, Coursera"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 12,
        "title": "Getting Started With Text Embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 8,
        "title": "Understanding Text Embeddings",
        "usesCodeExample": false
      },
      {
        "duration": 9,
        "title": "Visualizing Embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Applications of Embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Text Generation with Vertex AI",
        "usesCodeExample": true
      },
      {
        "duration": 19,
        "title": "Building a Q&A System Using Semantic Search",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Optional - Google Cloud Setup",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 76,
    "description": "Large Language Models (LLMs) are enabling coders and non-coders to build new kinds of applications that harness the power of AI. In this course, you’ll learn how to use and create with Microsoft’s open source orchestrator, Semantic Kernel. Along the way you’ll gain skills in getting the most out of LLMs developing prompts, semantic functions, vector databases and using an LLM for planning. After completing this course you will be able to: Develop sophisticated business applications using LLMs; Leverage common LLM building blocks such as memories, connectors, chains and planners; Utilize the open source orchestrator, the Semantic Kernel, in your applications. Using an orchestration SDK such as Semantic Kernel means you can avoid having to learn APIs for each individual AI service, and build integrations that can always stay up to date with the latest advances in AI research rather than solutions that quickly get outdated. Through this course, you’ll have all you need to take full advantage of this powerful open source tool.",
    "url": "https://www.deeplearning.ai/short-courses/microsoft-semantic-kernel",
    "title": "How Business Thinkers Can Start Building AI Plugins With Semantic Kernel",
    "studentProfile": "Everyone who wants to learn Semantic Kernel. Recommended skills are basic Python, and an understanding of an Application Programming Interface (API). Familiarity with what a Software Design Kit (SDK) is will be helpful, but not required.",
    "learningGoals": [
      "Learn Microsoft’s open source orchestrator, the Semantic Kernel",
      "Develop your business planning and analysis skills while leveraging AI tools",
      "Advance your skills in LLMs by using memories, connectors, chains, and more"
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "John Maeda",
        "title": "VP of Design and Artificial Intelligence at Microsoft"
      }
    ],
    "courseItems": [
      {
        "duration": 6,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 6,
        "title": "Semantic Kernel is Like Your AI Cooking Kitchen",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Cooking Up Flavorful SWOTs with the Kernel",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Organizing The Tools You Make for Later Reuse",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "Frozen Dinner The Design Thinking Meal",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Dont Forget to Save the Generated Dripping or The Gravy",
        "usesCodeExample": true
      },
      {
        "duration": 13,
        "title": "A Kitchen That Responds to Your I'm Hungry is More Than Feasible",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "There's a Fully-Outfitted Professional-Grade Kitchen Ready For You",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 85,
    "description": "Join our new short course, Finetuning Large Language Models! Learn from Sharon Zhou, Co-Founder and CEO of Lamini, and instructor for the GANs Specialization and How Diffusion Models Work. When you complete this course, you will be able to: Understand when to apply finetuning on LLMs, Prepare your data for finetuning, Train and evaluate an LLM on your data. With finetuning, you’re able to take your own data to train the model on it, and update the weights of the neural nets in the LLM, changing the model compared to other methods like prompt engineering and Retrieval Augmented Generation. Finetuning allows the model to learn style, form, and can update the model with new knowledge to improve results.",
    "url": "https://www.deeplearning.ai/short-courses/finetuning-large-language-models",
    "title": "Finetuning Large Language Models",
    "studentProfile": "Learners who want to understand the techniques and applications of finetuning, with Python familiarity, and an understanding of a deep learning framework such as PyTorch.",
    "learningGoals": [
      "Learn the fundamentals of finetuning a large language model (LLM).",
      "Understand how finetuning differs from prompt engineering, and when to use both.",
      "Get practical experience with real data sets, and how to use techniques for your own projects."
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Sharon Zhou",
        "title": "Co-Founder and CEO of Lamini"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Why finetune",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Where finetuning fits in",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Instruction finetuning",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "Data preparation",
        "usesCodeExample": true
      },
      {
        "duration": 16,
        "title": "Training process",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Evaluation and iteration",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Consideration on getting started now",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 72,
    "description": "Keyword search has been a common method for search for many years. But for content-rich websites like news media sites or online shopping platforms, the keyword search capability can be limiting. Incorporating large language models (LLMs) into your search can significantly enhance the user experience by allowing them to ask questions and find information in a much easier way.\n\nThis course teaches the techniques needed to leverage LLMs into search.\n\nThroughout the lessons, you’ll explore key concepts like dense retrieval, which elevates the relevance of retrieved information, leading to improved search results beyond traditional keyword search, and reranking, which injects the intelligence of LLMs into your search system, making it faster and more effective.\n\nAfter completing the course, you will:\n- Know how to implement basic keyword search, the underpinnings of many search systems before language models became accessible.\n- Enhance keyword search with the rerank method, which ranks the best responses by relevance with the query.\n- Implement dense retrieval through the use of embeddings, a powerful NLP tool, which uses the actual semantic meaning of the text to carry out search, and vastly improves results.\n- Gain hands-on practice by working with large amounts of data and overcome challenges like varying search results and accuracy.\n- Implement language model-powered search into your website or project.",
    "url": "https://www.deeplearning.ai/short-courses/large-language-models-semantic-search",
    "title": "Large Language Models with Semantic Search",
    "studentProfile": "Anyone who has basic familiarity with Python and wants to get a deeper understanding of key technical foundations of LLMs, and learn to use semantic search.",
    "learningGoals": [
      "Enhance keyword search using Cohere Rerank",
      "Use embeddings to leverage dense retrieval, a powerful NLP tool",
      "Evaluate your effectiveness for further optimization"
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Jay Alammar",
        "title": "Director and Engineering Fellow at Cohere and co-author of Hands-On Large Language Models"
      },
      {
        "name": "Luis Serrano",
        "title": "Lead of Developer Relations at Cohere"
      }
    ],
    "courseItems": [
      {
        "duration": 4,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "Keyword Search",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Embeddings",
        "usesCodeExample": true
      },
      {
        "duration": 20,
        "title": "Dense Retrieval",
        "usesCodeExample": true
      },
      {
        "duration": 10,
        "title": "ReRank",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Generating Answers",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 50,
    "description": "Machine learning and AI projects require managing diverse data sources, vast data volumes, model and parameter development, and conducting numerous test and evaluation experiments. Overseeing and tracking these aspects of a program can quickly become an overwhelming task.\n\nThis course will introduce you to Machine Learning Operations tools that manage this workload. You will learn to use the Weights & Biases platform which makes it easy to track your experiments, run and version your data, and collaborate with your team.\n\nThis course will teach you to:\n\n- Instrument a Jupyter notebook\n- Manage hyperparameter config\n- Log run metrics\n- Collect artifacts for dataset and model versioning\n- Log experiment results\n- Trace prompts and responses to LLMs over time in complex interactions\n\nWhen you complete this course, you will have a systematic workflow at your disposal to boost your productivity and accelerate your journey toward breakthrough results.",
    "url": "https://www.deeplearning.ai/short-courses/evaluating-debugging-generative-ai",
    "title": "Evaluating and Debugging Generative AI Models Using Weights and Biases",
    "studentProfile": "Anyone who has familiarity with Python and PyTorch or similar framework and an interest in managing, versioning, and debugging their machine learning workflow.",
    "learningGoals": [
      "Learn to evaluate programs utilizing LLMs as well as generative image models using platform-independent tools",
      "Instrument a training notebook, and add tracking, versioning, and logging",
      "Implement monitoring and tracing of LLMs over time in complex interactions"
    ],
    "difficulty": "Intermediate",
    "instructors": [
      {
        "name": "Carey Phelps",
        "title": "Founding Product Manager, Weights & Biases"
      }
    ],
    "courseItems": [
      {
        "duration": 3,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 10,
        "title": "Instrument W&B",
        "usesCodeExample": true
      },
      {
        "duration": 5,
        "title": "Training a Diffusion Model with W&B",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Evaluating Diffusion Models",
        "usesCodeExample": true
      },
      {
        "duration": 14,
        "title": "LLM Evaluation and Tracing with W&B",
        "usesCodeExample": true
      },
      {
        "duration": 7,
        "title": "Finetuning a language model",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 49,
    "description": "Join our new short course, Building Generative AI Applications with Gradio! Learn from Apolinário Passos, Machine Learning Art Engineer at Hugging Face.\n\nWhat you’ll do:\n\n- With a few lines of code, create a user-friendly app (usable for non-coders) to take input text, summarize it with an open-source large language model, and display the summary.\n- Create an app that allows the user to upload an image, which uses an image to text (image captioning) to describe the uploaded image, and display both the image and the caption in the app.\n- Create an app that takes text and generates an image with a diffusion model, then displays the generated image within the app.\n- Combine what you learned in the previous two lessons: Upload an image, caption the image, and use the caption to generate a new image.\n- Create an interface to chat with an open source LLM using Falcon, the best-ranking open source LLM on the Open LLM Leaderboard.\n\nBy the end of the course, you’ll gain the practical knowledge to rapidly build interactive apps and demos to validate your project and ship faster.",
    "url": "https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio",
    "title": "Building Generative AI Applications with Gradio",
    "studentProfile": "Anyone who has basic Python knowledge and wants to learn to quickly build and share apps and demos using Gradio!",
    "learningGoals": [
      "Learn how to create and demo machine learning applications quickly.",
      "Build your own image generation, image captioning, and text-summarization apps.",
      "Share your app with teammates, beta testers, and more on Hugging Face Spaces."
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Apolinário Passos",
        "title": "Machine Learning Art Engineer at Hugging Face"
      }
    ],
    "courseItems": [
      {
        "duration": 1,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 14,
        "title": "NLP Tasks interface",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Image Captioning app",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Image generation app",
        "usesCodeExample": true
      },
      {
        "duration": 4,
        "title": "Describe and Generate Game",
        "usesCodeExample": true
      },
      {
        "duration": 12,
        "title": "Chat with any LLM",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  },
  {
    "duration": 68,
    "description": "Join our new short course, LangChain: Chat With Your Data! The course delves into two main topics: (1) Retrieval Augmented Generation (RAG), a common LLM application that retrieves contextual documents from an external dataset, and (2) a guide to building a chatbot that responds to queries based on the content of your documents, rather than the information it has learned in training. You’ll learn about Document Loading, Document Splitting, Vector stores and embeddings, Retrieval, Question Answering, and Chat to build your own chatbot using LangChain.",
    "url": "https://www.deeplearning.ai/short-courses/langchain-chat-with-your-data",
    "title": "LangChain: Chat with Your Data",
    "studentProfile": "Developers familiar with Python who are interested in developing applications using Large Language Models like ChatGPT",
    "learningGoals": [
      "Learn directly from LangChain creator, Harrison Chase",
      "Access over 80 unique loaders to handle accessing various data sources using LangChain",
      "Build your own chatbot to chat directly with information from your own documents and data"
    ],
    "difficulty": "Beginner",
    "instructors": [
      {
        "name": "Harrison Chase",
        "title": "Co-Founder and CEO of LangChain"
      }
    ],
    "courseItems": [
      {
        "duration": 2,
        "title": "Introduction",
        "usesCodeExample": false
      },
      {
        "duration": 7,
        "title": "Document Loading",
        "usesCodeExample": true
      },
      {
        "duration": 15,
        "title": "Document Splitting",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Vectorstores and Embedding",
        "usesCodeExample": true
      },
      {
        "duration": 11,
        "title": "Retrieval",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Question Answering",
        "usesCodeExample": true
      },
      {
        "duration": 9,
        "title": "Chat",
        "usesCodeExample": true
      },
      {
        "duration": 1,
        "title": "Conclusion",
        "usesCodeExample": false
      },
      {
        "duration": 1,
        "title": "Quiz",
        "usesCodeExample": false
      }
    ],
    "usesCodeExamples": true
  }
]